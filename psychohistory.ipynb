{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3453dcd5-94d4-41fc-97d9-a7410e0ab1b1",
   "metadata": {},
   "source": [
    "# Long-Context Gemini: Unlocking Psychohistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d91aaa-9afd-4e93-8072-3a18430dc4ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## what is Psychohistory\n",
    "- Psychohistory is a fictional discipline introduced by science fiction author Isaac Asimov in the Foundation series. It combines history, sociology, statistics, and mathematics to analyze the behavior of large groups of individuals and predict trends in collective behavior and the future development of society. \n",
    "- While it cannot predict the actions of a single individual, the randomness of individual behaviors cancels out in sufficiently large groups, resulting in predictable patterns of overall societal behavior. \n",
    "- Psychohistory aims to provide a scientific basis for policymaking, forecasting social changes, and managing crises through mathematical models and big data analysis to predict the future of societies probabilistically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37636971-54e0-42e9-acf5-9ca772603452",
   "metadata": {},
   "source": [
    "## key point of Psychohistory\n",
    "- Simulation of Individual Psychological Activities and Actions:\n",
    "Psychohistory enables the simulation of individual human psychological processes and behaviors. By modeling the decision-making, emotions, and actions of numerous individuals, it captures the nuanced dynamics of human behavior in various contexts.\n",
    "- Aggregation of Individual Behaviors into Collective Psychological and Social Dynamics:\n",
    "From the vast array of individual psychological activities and actions, psychohistory synthesizes and summarizes the collective psychological state and social behaviors of groups. This aggregation allows for the prediction of large-scale societal trends and events based on the interplay of individual actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cbd0d6-c4bb-4ff5-8b62-e0d0371fcf5a",
   "metadata": {},
   "source": [
    "## why Gemini\n",
    "- Massive Human Data Training for Simulating Individual Psychological Activities and Actions:\n",
    "Gemini is trained on an extensive dataset comprising vast amounts of human-generated data. This extensive training enables it to accurately simulate individual psychological processes and behaviors, capturing the nuances of human decision-making, emotions, and actions with high fidelity.\n",
    "- Support for 1-2 Million Tokens Longcontext Mode to Summarize Collective Psychological Activities and Actions:\n",
    "Gemini’s longcontext mode, which supports 1-2 million tokens, allows it to process and analyze large-scale data efficiently. This capability enables Gemini to aggregate and summarize massive individual psychological activities and actions, facilitating the derivation of collective psychological states and societal behaviors essential for applications like psychohistory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c1d80-3447-40b9-bb19-7df4673f080d",
   "metadata": {},
   "source": [
    "## how to implement psychohistory using Gemini\n",
    "1.\tTo predict future states, use Gemini-1.5-Pro to insert several intermediate time points between the present and the future. While it is difficult to determine distant events, it is easier to make confident predictions about near-term events.（function：predict_future_sequence）\n",
    "2.\tTo predict future states, use Gemini-1.5-Pro to analyze which human physical and psychological states influence these states. Analyze these two sets of states at each time point.（function：predict_future_sequence）\n",
    "3.\tBased on the physical and psychological states of interest, use Gemini-1.5-Pro to design keywords for crawling data from the internet. Perform data crawling based on these keywords.（function：get_news_keywords_for_topic，get_reddit_subreddits_for_topic）\n",
    "4.\tUse Gemini-1.5-Flash to analyze each piece of data for its impact on the states, and use LongContext Gemini to summarize the two sets of attributes (currently representing the attributes of the real world).（function：answer_questions，class：CachedDeriveOverallState）\n",
    "\n",
    "For loop：With the two sets of attributes from the current real-world state, predict the two sets of attributes for the next time point:\n",
    "- Based on the current physical and psychological states of the world, use Gemini to simulate individual actions and analyze how these actions impact the next time point.（function：predict_individual_future_actions）\n",
    "- Aggregate all these impacts into a long text and use Gemini LongContext to summarize them, obtaining the physical attributes for the next time point (analogous to using distance, speed, and time to calculate the next distance).（function：forecast_overall_future_state）\n",
    "- With the physical attributes of the next time point, use Gemini to simulate news reports and online discussions, effectively simulating psychological activities.（function：simulate_future_individual_behavior）\n",
    "- Use Gemini-1.5-Flash to analyze each news report or online discussion for its impact on psychological states.（function：answer_questions）\n",
    "- Aggregate all these impacts into a long text and use Gemini LongContext to summarize them, obtaining the psychological attributes for the next time point.（function：derive_overall_state）\n",
    "- With both sets of attributes for one time point, move to the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e583ab7-3a2f-4421-9bb8-342556279255",
   "metadata": {},
   "source": [
    "## why LongContext Gemini is so important for Psychohistory\n",
    "- As mentioned earlier, a key aspect of psychohistory is the ability to summarize group states from the psychological activities or actions of numerous individuals. For example, if describing one individual’s psychological activity requires 300 tokens of text, a group of 2,000 people would need a context length of 600,000 tokens. The larger the context window, the larger the group that can be supported. The larger the group, the smaller the impact of individual randomness on the group.\n",
    "- In the CachedDeriveOverallState class, LongContext Gemini must be used to summarize the physical and psychological states of the real world from massive amounts of internet data.\n",
    "- In the forecast_overall_future_state function, LongContext Gemini must be used to summarize the behaviors of a massive number of individuals and their impacts on a future time point to derive the physical state at that future time point.\n",
    "- In the derive_overall_state function, LongContext Gemini must be used to summarize the simulated psychological activities of a massive number of individuals to determine the psychological state at a given time point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869af19-a793-4559-a97e-bb7242473647",
   "metadata": {},
   "source": [
    "## why caching\n",
    "The API cost for LongContext is relatively high. When handling large shard prompts and requiring repeated requests, the Context-Caching mode can be used to reduce expenses.\n",
    "In this project, the Context-Caching mode is implemented in the CachedDeriveOverallState class to cache the vast amount of internet text data. This cached data is then repeatedly used for requests to summarize the current physical and psychological state of society."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34f11a-6fee-4e9d-89d9-583a50fc3c93",
   "metadata": {},
   "source": [
    "## Three execution results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4674fa6e-f16c-4b6e-a55d-662d927f0cd0",
   "metadata": {},
   "source": [
    "Q1: What level of development will artificial intelligence reach in the United States by 2050?\n",
    "Psychohistory: By 2050, artificial intelligence in the United States has reached pervasive integration and advanced capability, significantly transforming society.  While not achieving true sentience, AI exhibits sophisticated problem-solving across numerous sectors.  This is evidenced by ubiquitous deployment in healthcare (80% of hospitals using AI diagnostics, 100 million telehealth patients), education (70% of schools using AI platforms), finance ($220 trillion in AI-driven trading), and infrastructure (70% AI-managed).  Massive investment ($230 trillion private, $35 billion federal) fuels a robust AI industry (30,000 startups, 2.5 million patents) supported by substantial computing power (170 ExaFLOPS).  Despite regulatory efforts (400 AI-specific laws), concerns persist regarding cybersecurity incidents (120,000 reported) and data privacy (3,000 lawsuits).  The impact on employment remains relatively low (3.8% unemployment in impacted industries).  International collaboration (500 joint projects) and continued military application ($32 billion spending) further underscore AI's central role in shaping the future.  In short, AI has become a powerful, if not perfectly controlled, force shaping American life.\n",
    "\n",
    "Q2: What level of advancement will SpaceX’s space technology reach by 2050?\n",
    "Psychohistory:SpaceX, by 2050, transitioned from a launch provider to a vertically integrated space industrial behemoth.  They dominate reusable launch, orbital infrastructure, in-space resource utilization, and interplanetary transport.  Their reusable architecture, refined over hundreds of launches, drives down costs, enabling the burgeoning space economy evidenced by the trillion-dollar space tourism industry and extensive private space investment.  While no longer the sole player, their legacy is the normalization of space access, paving the way for widespread industrialization and the beginnings of a true spacefaring civilization.\n",
    "\n",
    "Q3: What will the political polarization between the two parties in the United States develop into by 2050?\n",
    "Psychohistory: Political polarization in the United States has solidified into a persistent, low-turnout stalemate.  While near parity exists in party identification (41% Republican, 40% Democrat),  high levels of partisan gerrymandering (28% of districts) and massive campaign finance expenditures ($9.2B total, with minimal independent spending) reinforce existing divisions. This, coupled with pervasive misinformation (affecting 52% of fact-checked news) and an extremely concentrated media landscape (HHI 1620),  creates echo chambers that deepen ideological divides.  While voter turnout has marginally improved across all affiliations (61-64%), it isn't enough to overcome structural advantages.  The result is a narrowly divided government prone to gridlock, reflected in low approval ratings for national institutions (President: 35%, Congress: 28%).  Increasingly, political action manifests not through electoral change, but through lobbying ($9B) and, disturbingly,  a rise in politically motivated violence (750 incidents).  This entrenched polarization overshadows pressing issues like climate change (1.2°C warming, 95 extreme weather events) and economic inequality (Gini 0.44),  hindering meaningful solutions and fueling public disillusionment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a397b-7467-4b96-97e8-33ddc81ef6a9",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "- The data collected from the internet is limited to news snippets and Reddit posts, lacking diversity in sources.\n",
    "- Experiments with a 2M context length have not yet been conducted.\n",
    "- Currently, only supports questions like “Will something happen by year XX?” and does not support questions like “When will something happen?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0df5c6-80a9-48b0-b680-b840da79a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import praw\n",
    "from datetime import datetime,timedelta \n",
    "from newsapi import NewsApiClient\n",
    "import concurrent.futures\n",
    "from typing import Any\n",
    "import threading\n",
    "from google.generativeai import caching\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import time\n",
    "import random\n",
    "\n",
    "# newsapi 是新闻爬取包，使用pip install newsapi-python安装。https://newsapi.org/\n",
    "# praw 是reddit爬取包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee67b31-6a0a-4d2a-af12-b65cc4b287b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit api setting\n",
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "user_agent = \"\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# newsapi key setting\n",
    "news_api_key = \"\"\n",
    "newsapi = NewsApiClient(api_key=news_api_key)\n",
    "\n",
    "# gemini key\n",
    "genai.configure(api_key=os.environ['API_KEY'])\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "usage_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9d3715-067b-4528-b442-ec34a09b3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_posts_file(path):\n",
    "    \"\"\"\n",
    "    Reads JSON files containing lists of strings (each string is a combined article or post),\n",
    "    and returns a concatenated list of strings.\n",
    "\n",
    "    Parameters:\n",
    "    - path: either a folder path (string) or a list of file paths (list of strings)\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    if isinstance(path, str):  # It's a folder path\n",
    "        json_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.json')]\n",
    "    elif isinstance(path, list):  # It's a list of file paths\n",
    "        json_files = path\n",
    "    else:\n",
    "        raise ValueError(\"path must be a folder path (string) or a list of file paths (list of strings)\")\n",
    "\n",
    "    for file_path in json_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            all_data.extend(data)\n",
    "    return all_data\n",
    "\n",
    "def get_token_count(text,model):\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model)\n",
    "        response = model.count_tokens(text)\n",
    "        return response.total_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting tokens: {e}\")\n",
    "        return 0\n",
    "\n",
    "def log_usage_data(op_name, model_name, prompt_tokens, candidates_tokens, cached_content_tokens=None, duration_seconds=None, usage_log_file='usage_log.json'):\n",
    "    \"\"\"\n",
    "    Logs the usage data into a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - op_name: Name of the llm op.\n",
    "    - model_name: Name of the model used.\n",
    "    - prompt_tokens: Number of prompt tokens.\n",
    "    - candidates_tokens: Number of candidates tokens.\n",
    "    - cached_content_tokens: Number of cached content tokens (if any).\n",
    "    - duration_seconds: Duration in seconds (if applicable).\n",
    "    - usage_log_file: Path to the usage log file.\n",
    "    \"\"\"\n",
    "    usage_data = {\n",
    "        'op_name': op_name,\n",
    "        'model_name': model_name,\n",
    "        'prompt_tokens': prompt_tokens,\n",
    "        'candidates_tokens': candidates_tokens,\n",
    "    }\n",
    "    if cached_content_tokens is not None:\n",
    "        usage_data['cached_content_tokens'] = cached_content_tokens\n",
    "    if duration_seconds is not None:\n",
    "        usage_data['duration_seconds'] = duration_seconds\n",
    "\n",
    "    with usage_lock:\n",
    "        if os.path.exists(usage_log_file):\n",
    "            with open(usage_log_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = []\n",
    "\n",
    "        data.append(usage_data)\n",
    "\n",
    "        with open(usage_log_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def select_posts_by_token_limit(json_path, target_tokens, model=\"gemini-pro\"):\n",
    "    \"\"\"\n",
    "    Select posts from a JSON file to limit total tokens to a specified number.\n",
    "\n",
    "    Parameters:\n",
    "    - json_path (str): Path to the JSON file containing a list of posts.\n",
    "    - target_tokens (int): The target token limit.\n",
    "    - model (str): The name of the model to use for token counting (default: \"gemini-pro\").\n",
    "\n",
    "    Returns:\n",
    "    - list: Selected posts within the token limit.\n",
    "    \"\"\"\n",
    "    # Read the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        posts = json.load(f)\n",
    "\n",
    "    # Concatenate posts and calculate initial token count\n",
    "    concatenated_posts = \"\\n\".join(posts)\n",
    "    total_tokens = get_token_count(concatenated_posts, model)\n",
    "    total_posts = len(posts)\n",
    "\n",
    "    # print(f\"Initial total tokens: {total_tokens}, Total posts: {total_posts}\")\n",
    "\n",
    "    # If tokens exceed the target, randomly sample posts\n",
    "    if total_tokens > target_tokens * 0.95:\n",
    "        # Calculate the fraction of posts to sample\n",
    "        sample_fraction = (target_tokens * 0.90) / total_tokens\n",
    "        sample_count = max(1, int(sample_fraction * total_posts))\n",
    "\n",
    "        # print(f\"Sampling {sample_count} posts out of {total_posts}\")\n",
    "\n",
    "        # Randomly sample posts\n",
    "        sampled_posts = random.sample(posts, sample_count)\n",
    "\n",
    "        # Verify the new token count\n",
    "        sampled_concatenated_posts = \"\\n\".join(sampled_posts)\n",
    "        new_token_count = get_token_count(sampled_concatenated_posts, model)\n",
    "\n",
    "        # print(f\"New total tokens after sampling: {new_token_count}\")\n",
    "\n",
    "        # Return sampled posts if within the limit\n",
    "        if new_token_count <= target_tokens * 0.95:\n",
    "            return sampled_posts\n",
    "        else:\n",
    "            # print(f\"Warning: Token count still exceeds {target_tokens * 0.95}. Returning sampled posts.\")\n",
    "            return sampled_posts\n",
    "    else:\n",
    "        # If tokens are already within the limit, return all posts\n",
    "        # print(f\"Token count is within limit. Returning all posts.\")\n",
    "        return posts\n",
    "\n",
    "def get_subreddit_posts(keyword, print_data=False, sort_by='hot', limit=10, max_dialogues_per_comment=5, save_to_file=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Fetches posts from a specified subreddit and optionally saves them to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - keyword (str): The name of the subreddit to fetch posts from.\n",
    "    - print_data (bool, optional): If True, prints the fetched data to the console. Default is False.\n",
    "    - sort_by (str, optional): Sorting method for posts; either 'hot' or 'new'. Default is 'hot'.\n",
    "    - limit (int, optional): The maximum number of posts to fetch. Default is 10.\n",
    "    - max_dialogues_per_comment (int, optional): The maximum number of dialogues to collect per comment thread. Default is 5.\n",
    "    - save_to_file (bool, optional): If True, saves the fetched data to a file. Default is False.\n",
    "    - save_path (str, optional): The directory path where the data should be saved. If None, defaults to \"collect_data\".\n",
    "\n",
    "    Returns:\n",
    "    - List[dict]: A list of dictionaries containing information about each fetched post.\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(keyword)\n",
    "    \n",
    "    # Select sorting method based on sort_by parameter\n",
    "    if sort_by == 'hot':\n",
    "        posts_iter = subreddit.hot(limit=limit)\n",
    "    elif sort_by == 'new':\n",
    "        posts_iter = subreddit.new(limit=limit)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid sort_by value. Use 'hot' or 'new'.\")\n",
    "    \n",
    "    posts = []\n",
    "    for post in posts_iter:\n",
    "        comments = []\n",
    "\n",
    "        def collect_comments(comment_list, current_depth=0, max_depth=5, dialogues_collected=0, max_dialogues=10):\n",
    "            result = []\n",
    "            if current_depth >= max_depth or dialogues_collected >= max_dialogues:\n",
    "                return result, dialogues_collected\n",
    "\n",
    "            for comment in comment_list:\n",
    "                if isinstance(comment, praw.models.MoreComments):\n",
    "                    continue\n",
    "\n",
    "                comment_info = {\n",
    "                    \"Comment\": comment.body.replace('\\n', ' '),\n",
    "                    \"Replies\": []\n",
    "                }\n",
    "                dialogues_collected += 1\n",
    "\n",
    "                if dialogues_collected >= max_dialogues:\n",
    "                    result.append(comment_info)\n",
    "                    break\n",
    "\n",
    "                replies, dialogues_collected = collect_comments(\n",
    "                    comment.replies,\n",
    "                    current_depth=current_depth+1,\n",
    "                    max_depth=max_depth,\n",
    "                    dialogues_collected=dialogues_collected,\n",
    "                    max_dialogues=max_dialogues\n",
    "                )\n",
    "                comment_info[\"Replies\"] = replies\n",
    "                result.append(comment_info)\n",
    "\n",
    "                if dialogues_collected >= max_dialogues:\n",
    "                    break\n",
    "\n",
    "            return result, dialogues_collected\n",
    "\n",
    "        post_comments, _ = collect_comments(\n",
    "            post.comments,\n",
    "            max_depth=5,\n",
    "            max_dialogues=max_dialogues_per_comment\n",
    "        )\n",
    "\n",
    "        post_info = {\n",
    "            \"Subreddit\": post.subreddit.display_name,\n",
    "            \"Title\": post.title.replace('\\n', ' '),\n",
    "            \"Content\": post.selftext.replace('\\n', ' '),\n",
    "            \"Upvotes (Score)\": post.score,\n",
    "            \"Author\": str(post.author),\n",
    "            \"Number of Comments\": post.num_comments,\n",
    "            \"Created Date\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"Post URL\": post.url,\n",
    "            \"CommentList\": post_comments\n",
    "        }\n",
    "        posts.append(post_info)\n",
    "        \n",
    "        if print_data:\n",
    "            title_clean = post.title.replace('\\n', ' ')\n",
    "            content_clean = post.selftext.replace('\\n', ' ')\n",
    "            subreddit_name = post.subreddit.display_name\n",
    "            \n",
    "            print(f\"Subreddit: {subreddit_name}\")\n",
    "            print(f\"Post Title: {title_clean}\")\n",
    "            print(f\"Post Content: {content_clean}\")\n",
    "            print(f\"Upvotes (Score): {post.score}\")\n",
    "            print(f\"Post URL: {post.url}\")\n",
    "            print(f\"Author: {post.author}\")\n",
    "            print(f\"Number of Comments: {post.num_comments}\")\n",
    "            created_time = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"Created Date: {created_time}\")\n",
    "            print(\"Comments:\")\n",
    "            \n",
    "            def print_comments(comments_list, indent=0):\n",
    "                for c in comments_list:\n",
    "                    comment_clean = c['Comment'].replace('\\n', ' ')\n",
    "                    print(\" \" * indent + f\"- {comment_clean}\")\n",
    "                    if c['Replies']:\n",
    "                        print_comments(c['Replies'], indent + 4)\n",
    "\n",
    "            print_comments(post_comments)\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    if save_to_file:\n",
    "        if save_path is None:\n",
    "            save_path = \"collect_data\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        date_str = datetime.now().strftime('%Y%m%d')\n",
    "        filename = f\"{save_path}/reddit_{keyword}_{sort_by}_{limit}_{date_str}.json\"\n",
    "\n",
    "        # Create a list to hold the combined post strings\n",
    "        posts_strings = []\n",
    "\n",
    "        for post in posts:\n",
    "            post_string = (\n",
    "                f\"Subreddit: {post['Subreddit']}\\n\"\n",
    "                f\"Title: {post['Title']}\\n\"\n",
    "                f\"Content: {post['Content']}\\n\"\n",
    "                f\"Upvotes (Score): {post['Upvotes (Score)']}\\n\"\n",
    "                f\"Author: {post['Author']}\\n\"\n",
    "                f\"Number of Comments: {post['Number of Comments']}\\n\"\n",
    "                f\"Created Date: {post['Created Date']}\\n\"\n",
    "                f\"Post URL: {post['Post URL']}\\n\"\n",
    "                f\"Comments:\\n\"\n",
    "            )\n",
    "\n",
    "            def format_comments(comments_list, indent=0):\n",
    "                comment_strings = []\n",
    "                for c in comments_list:\n",
    "                    comment_str = \" \" * indent + f\"- {c['Comment']}\"\n",
    "                    comment_strings.append(comment_str)\n",
    "                    if c['Replies']:\n",
    "                        comment_strings.extend(format_comments(c['Replies'], indent + 4))\n",
    "                return comment_strings\n",
    "\n",
    "            comments_strings = format_comments(post['CommentList'])\n",
    "            post_string += \"\\n\".join(comments_strings)\n",
    "\n",
    "            # Append the combined post string to the list\n",
    "            posts_strings.append(post_string)\n",
    "\n",
    "        # Save the list of post strings to a JSON file\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts_strings, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    return posts\n",
    "\n",
    "\n",
    "def get_news_articles(keywords, from_date=None, to_date=None, total_pages=1, print_data=False, page_size=20, save_to_file=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Fetches news articles based on provided keywords and optional date range, and optionally saves them to files.\n",
    "\n",
    "    Parameters:\n",
    "    - keywords (List[str]): A list of keywords to search for in news articles.\n",
    "    - from_date (str, optional): The start date for fetching articles in 'YYYY-MM-DD' format. If None, no start date filter is applied.\n",
    "    - to_date (str, optional): The end date for fetching articles in 'YYYY-MM-DD' format. If None, no end date filter is applied.\n",
    "    - total_pages (int, optional): The number of pages to fetch for each keyword. Default is 1.\n",
    "    - print_data (bool, optional): If True, prints the fetched articles to the console. Default is False.\n",
    "    - page_size (int, optional): The number of articles per page. Default is 20.\n",
    "    - save_to_file (bool, optional): If True, saves the fetched articles to files. Default is False.\n",
    "    - save_path (str, optional): The directory path where the articles should be saved. If None, defaults to \"collect_data\".\n",
    "\n",
    "    Returns:\n",
    "    - List[dict]: A list of dictionaries containing information about each fetched article.\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    for keyword in keywords:\n",
    "        # print(f\"Fetching articles for keyword: {keyword}\")\n",
    "        articles_for_keyword = []  # Initialize list for current keyword\n",
    "\n",
    "        for page in range(1, total_pages + 1):\n",
    "            try:\n",
    "                # Fetch articles with pagination and date filters\n",
    "                articles = newsapi.get_everything(\n",
    "                    q=keyword,\n",
    "                    language='en',\n",
    "                    page_size=page_size,\n",
    "                    page=page,\n",
    "                    from_param=from_date,\n",
    "                    to=to_date\n",
    "                )\n",
    "\n",
    "                # Add exception handling based on the response status\n",
    "                if articles['status'] != 'ok':\n",
    "                    # print(f\"Error fetching articles for keyword '{keyword}': {articles.get('message', 'Unknown error')}\")\n",
    "                    continue\n",
    "\n",
    "                # Process articles as before\n",
    "                for article in articles['articles']:\n",
    "                    article_info = {\n",
    "                        \"Source\": article['source']['name'],\n",
    "                        \"Author\": article['author'],\n",
    "                        \"Title\": article['title'],\n",
    "                        \"Description\": article['description'],\n",
    "                        \"Content\": article['content'],\n",
    "                        \"Published At\": article['publishedAt'],\n",
    "                        \"URL\": article['url'],\n",
    "                    }\n",
    "                    articles_for_keyword.append(article_info)\n",
    "                    all_articles.append(article_info)\n",
    "\n",
    "                    if print_data:\n",
    "                        print(f\"Source: {article_info['Source']}\")\n",
    "                        print(f\"Author: {article_info['Author']}\")\n",
    "                        print(f\"Title: {article_info['Title']}\")\n",
    "                        print(f\"Description: {article_info['Description']}\")\n",
    "                        print(f\"Content: {article_info['Content']}\")\n",
    "                        print(f\"Published At: {article_info['Published At']}\")\n",
    "                        print(f\"URL: {article_info['URL']}\")\n",
    "                        print(\"-\" * 50)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred while fetching articles for keyword '{keyword}', page {page}: {e}\")\n",
    "\n",
    "        # Save articles for the current keyword to a separate file\n",
    "        if save_to_file:\n",
    "            if save_path is None:\n",
    "                save_path = \"collect_data\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "            date_str = datetime.now().strftime('%Y%m%d')\n",
    "            filename = f\"{save_path}/news_{keyword}_{page_size}_{date_str}.json\"\n",
    "\n",
    "            # Create a list to hold the combined article strings\n",
    "            articles_strings = []\n",
    "\n",
    "            for article in articles_for_keyword:\n",
    "                article_string = (\n",
    "                    f\"Source: {article['Source']}\\n\"\n",
    "                    f\"Author: {article['Author']}\\n\"\n",
    "                    f\"Title: {article['Title']}\\n\"\n",
    "                    f\"Description: {article['Description']}\\n\"\n",
    "                    f\"Content: {article['Content']}\\n\"\n",
    "                    f\"Published At: {article['Published At']}\\n\"\n",
    "                )\n",
    "                articles_strings.append(article_string)\n",
    "\n",
    "            # Save the list of article strings to a JSON file\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(articles_strings, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d22b9aa-1cf9-4a8c-a403-cef803dfe438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_keywords_for_topic(topic, questions=None):\n",
    "    \"\"\"\n",
    "    Generates a list of broad and general keywords for news article searches related to a given topic.\n",
    "\n",
    "    Parameters:\n",
    "    - topic (str): The main topic to generate keywords for.\n",
    "    - questions (List[str], optional): Additional questions to refine the keyword generation. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of broad and general keywords suitable for searching news articles.\n",
    "    \"\"\"\n",
    "    if questions is None:\n",
    "        questions = []\n",
    "\n",
    "    questions_text = \"\\n\".join(questions)\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "\n",
    "    # First LLM Request: Analysis and Summarization\n",
    "    prompt1 = f\"\"\"\n",
    "Given the following topic:\n",
    "{questions_text}\n",
    "\n",
    "The above topic require you to search for related information. Please carefully analyze what keywords can be used in search engines to obtain broad, general information related to these topic.\n",
    "\n",
    "1. **Avoid overly specific terms.** Instead of exact phrases (e.g., \"US renewable energy capacity 2023 EIA\"), use broader terms (e.g., \"US renewable energy\").\n",
    "2. **Focus on thematic keywords**: Extract high-level thematic fields (e.g., \"US economy,\" \"US environment,\" \"US technology\").\n",
    "3. **Identify broad keyword combinations**: Consider general keywords that capture the overall intent of the questions without limiting the scope unnecessarily.\n",
    "\n",
    "Finally, generate a list of **broad and general keyword combinations** suitable for searching across major news databases or online platforms.\n",
    "\"\"\"\n",
    "\n",
    "    # Generate response for first request\n",
    "    response1 = model.generate_content(prompt1)\n",
    "    usage1 = response1.usage_metadata\n",
    "    input_tokens1 = usage1.prompt_token_count\n",
    "\n",
    "    # Log token usage for first request\n",
    "    # print(f\"Request: get_news_keywords_for_topic (first call), Tokens used - Input: {input_tokens1}, Output: {usage1.candidates_token_count}, Total: {usage1.total_token_count}\")\n",
    "\n",
    "    # Extract the analysis text\n",
    "    analysis_text = response1._result.candidates[0].content.parts[0].text\n",
    "\n",
    "    # Second LLM Request: Extract Keywords as JSON\n",
    "    prompt2 = f\"\"\"\n",
    "{prompt1}\n",
    "{analysis_text}\n",
    "\n",
    "Based on the refined analysis, adjust the keywords to make them broader and more general. Avoid overly specific terms, and ensure each keyword reflects high-level thematic categories (e.g., \"US economy,\" \"climate change,\" \"global technology\"). \n",
    "\n",
    "Return a list of relevant, broad keywords as a JSON array of strings.\n",
    "\"\"\"\n",
    "\n",
    "    # Define the expected JSON schema\n",
    "    response_schema = {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\"type\": \"string\"}\n",
    "    }\n",
    "\n",
    "    # Generate response for second request\n",
    "    response2 = model.generate_content(\n",
    "        prompt2,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=response_schema\n",
    "        ),\n",
    "    )\n",
    "    usage2 = response2.usage_metadata\n",
    "    input_tokens2 = usage2.prompt_token_count\n",
    "\n",
    "    # Log token usage for second request\n",
    "    # print(f\"Request: get_news_keywords_for_topic (second call), Tokens used - Input: {input_tokens2}, Output: {usage2.candidates_token_count}, Total: {usage2.total_token_count}\")\n",
    "\n",
    "    # Parse the response content\n",
    "    text = response2._result.candidates[0].content.parts[0].text\n",
    "    keywords = json.loads(text)\n",
    "\n",
    "    # Save prompts and responses\n",
    "    prompttemp_dir = os.path.join(\"prompttemp\")\n",
    "    os.makedirs(prompttemp_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(prompttemp_dir, 'get_news_keywords_for_topic_prompt1.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== Prompt 1 ===\\n\")\n",
    "        f.write(prompt1)\n",
    "        f.write(\"\\n\\n=== Response 1 ===\\n\")\n",
    "        f.write(analysis_text)\n",
    "\n",
    "    with open(os.path.join(prompttemp_dir, 'get_news_keywords_for_topic_prompt2.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== Prompt 2 ===\\n\")\n",
    "        f.write(prompt2)\n",
    "        f.write(\"\\n\\n=== Response 2 ===\\n\")\n",
    "        f.write(json.dumps(keywords, indent=4))\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def get_reddit_subreddits_for_topic(topic, questions=None):\n",
    "    \"\"\"\n",
    "    Generates a list of relevant Reddit subreddit names based on a given topic.\n",
    "\n",
    "    Parameters:\n",
    "    - topic (str): The main topic to find subreddits for.\n",
    "    - questions (List[str], optional): Additional questions to refine the subreddit search. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of subreddit names related to the topic.\n",
    "    \"\"\"\n",
    "    if questions is None:\n",
    "        questions = []\n",
    "    \n",
    "    questions_text = \"\\n\".join(questions)\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "\n",
    "    # First LLM Request: Analysis and Summarization\n",
    "    prompt1 = f\"\"\"\n",
    "Given the following topics:\n",
    "{questions_text}\n",
    "\n",
    "You are searching for relevant user discussions on Reddit about the topics mentioned above. Please carefully analyze the fields involved in these topics and brainstorm related Reddit subreddit names.\n",
    "\t1.\tFirst, write down your analysis to determine the fields or themes related to these topics.\n",
    "\t2.\tBrainstorm which subreddits are likely to host discussions on these fields or themes, and list these subreddits.\n",
    "\t3.\tSelect the best 20 subreddits from this list.\n",
    "\"\"\"\n",
    "\n",
    "    # Generate response for first request\n",
    "    response1 = model.generate_content(prompt1)\n",
    "\n",
    "    # Log token usage for first request\n",
    "    usage1 = response1.usage_metadata\n",
    "    input_tokens1 = usage1.prompt_token_count\n",
    "    # print(f\"Request: get_reddit_subreddits_for_topic (first call), Tokens used - Input: {input_tokens1}, Output: {usage1.candidates_token_count}, Total: {usage1.total_token_count}\")\n",
    "\n",
    "    # Extract the analysis text\n",
    "    analysis_text = response1._result.candidates[0].content.parts[0].text\n",
    "\n",
    "    # Second LLM Request: Extract Subreddit Names as JSON\n",
    "    prompt2 = f\"\"\"\n",
    "{prompt1}\n",
    "{analysis_text}\n",
    "\n",
    "Please refine these last subreddit names. Ensure that each subreddit is relevant to the topic and questions, and consider any contextual details derived from the analysis.\n",
    "\n",
    "Return a list of subreddit names as a JSON array of strings.\n",
    "\"\"\"\n",
    "\n",
    "    # Define the expected JSON schema\n",
    "    response_schema = {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\"type\": \"string\"}\n",
    "    }\n",
    "\n",
    "    # Generate response for second request\n",
    "    response2 = model.generate_content(\n",
    "        prompt2,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=response_schema\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Log token usage for second request\n",
    "    usage2 = response2.usage_metadata\n",
    "    input_tokens2 = usage2.prompt_token_count\n",
    "    # print(f\"Request: get_reddit_subreddits_for_topic (second call), Tokens used - Input: {input_tokens2}, Output: {usage2.candidates_token_count}, Total: {usage2.total_token_count}\")\n",
    "\n",
    "    # Parse the response content\n",
    "    text = response2._result.candidates[0].content.parts[0].text\n",
    "    subreddits = json.loads(text)\n",
    "\n",
    "    # Save prompts and responses\n",
    "    prompttemp_dir = os.path.join(\"prompttemp\")\n",
    "    os.makedirs(prompttemp_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(prompttemp_dir, 'get_reddit_subreddits_for_topic_prompt1.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== Prompt 1 ===\\n\")\n",
    "        f.write(prompt1)\n",
    "        f.write(\"\\n\\n=== Response 1 ===\\n\")\n",
    "        f.write(analysis_text)\n",
    "\n",
    "    with open(os.path.join(prompttemp_dir, 'get_reddit_subreddits_for_topic_prompt2.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== Prompt 2 ===\\n\")\n",
    "        f.write(prompt2)\n",
    "        f.write(\"\\n\\n=== Response 2 ===\\n\")\n",
    "        f.write(json.dumps(subreddits, indent=4))\n",
    "\n",
    "    return subreddits\n",
    "\n",
    "def filter_post_by_questions(post_string, questions):\n",
    "    \"\"\"\n",
    "    Determines whether the post_string is related to the list of questions.\n",
    "\n",
    "    Parameters:\n",
    "    - post_string: The post content string.\n",
    "    - questions: The list of questions.\n",
    "\n",
    "    Returns:\n",
    "    - bool: Indicates whether it is related.\n",
    "    \"\"\"\n",
    "\n",
    "    questions_text = \"\\n\".join(questions)\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Given the following post:\n",
    "————————————————————\n",
    "{post_string}\n",
    "————————————————————\n",
    "And the following questions:\n",
    "————————————————————\n",
    "{questions_text}\n",
    "————————————————————\n",
    "Determine whether the post is related to any of the questions above. Return your answer as a JSON object with a single field \"is_related\", which is a boolean value.\n",
    "\n",
    "Ensure that you only output the JSON object and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "    input_tokens = model.count_tokens(prompt)\n",
    "\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"is_related\": {\"type\": \"boolean\"}\n",
    "                },\n",
    "                \"required\": [\"is_related\"]\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    usage = response.usage_metadata\n",
    "    # print(f\"Request: filter_post_by_questions, Tokens used - Input: {input_tokens}, Output: {usage.candidates_token_count}, Total: {usage.total_token_count}\")\n",
    "\n",
    "    text = response._result.candidates[0].content.parts[0].text\n",
    "    result = json.loads(text)\n",
    "    is_related = result.get(\"is_related\", False)\n",
    "\n",
    "    prompttemp_dir = os.path.join(\"prompttemp\")\n",
    "    os.makedirs(prompttemp_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(prompttemp_dir, 'filter_post_by_questions_prompt.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== Prompt ===\\n\")\n",
    "        f.write(prompt)\n",
    "        f.write(\"\\n\\n=== Response ===\\n\")\n",
    "        f.write(text)\n",
    "\n",
    "    return is_related\n",
    "\n",
    "def predict_future_sequence(user_request):\n",
    "    \"\"\"\n",
    "    Generates a sequence of future time points and key attributes based on the concept of psychohistory.\n",
    "\n",
    "    This function uses an LLM to analyze the user's request and divides the future time span into significant\n",
    "    time points. It also identifies important social and psychological attributes that will influence the event\n",
    "    described in the user request.\n",
    "\n",
    "    Parameters:\n",
    "    - user_request (str): The event or topic for which to predict the future sequence.\n",
    "\n",
    "    Returns:\n",
    "    - time_points (List[str]): A list of future time points as strings.\n",
    "    - physical_questions (List[str]): A list of social attributes to consider.\n",
    "    - psychological_questions (List[str]): A list of group psychological attributes to consider.\n",
    "    \"\"\"\n",
    "    # Capture the current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Updated instruction variable\n",
    "    instruction = f'''\n",
    "**You are tasked with using \"psychohistory\" to predict the future of humanity. Psychohistory, a concept introduced by science fiction author Isaac Asimov, predicts future human events by analyzing a large number of individual behaviors and psychological activities.**\n",
    "Predicting events far in the future is highly challenging, so you will insert a series of critical time points within the long time span from the present to the target event. You will progressively predict the states at these time points step by step until reaching the final time point.\n",
    "To predict an event, you need to understand certain attributes of the real world. Based on the provided event, analyze step by step the attributes you need to know.\n",
    "---\n",
    "### **Process for Dividing Time Points:**\n",
    "1. **Analysis of the Time Span**\n",
    "   Write an analysis of the entire period from the current date to the target future date. Based on existing patterns in human behavior, identify major events, policy changes, or natural progression intervals. Then, list a series of more than 20 time points (these time points should not include the starting or target time).\n",
    "2. **Select Key Time Points**\n",
    "   From the above candidate time points, select 5 relatively evenly distributed and significant time points.\n",
    "3. **Add Current and Target Dates**\n",
    "   Based on the selected time points, add the present date and the target future date to form a comprehensive timeline.\n",
    "---\n",
    "### **Process for Determining Key Attributes:**\n",
    "1. **Analysis of Key Attributes**\n",
    "   Write a detailed analysis based on the event to be predicted. Analyze which social attributes and group psychological attributes will affect the event.\n",
    "   - Select **20 social attributes**, focusing on measurable aspects of society (e.g., infrastructure, climate, health metrics).\n",
    "   - Select **20 group psychological attributes**, focusing on psychological activities of groups, such as attitudes, perceptions, and opinions about various aspects of life or events.\n",
    "2. **Select Important Attributes**\n",
    "   Choose the 20 most important attributes from each list, ensuring both social and group psychological attributes are included.\n",
    "3. **List the Attributes**\n",
    "   - **Social Attributes**: Provide a list of the selected social attributes.\n",
    "   - **Group Psychological Attributes**: Provide a list of the selected group psychological attributes.\n",
    "---\n",
    "\n",
    "### **Your Task:**\n",
    "\n",
    "You are now at {current_date}, going to predict the following event:\n",
    "\n",
    "**{user_request}**\n",
    "!!!!If the user’s prediction includes regional limitations (e.g., US, Hong Kong), please add these regional limitations to the required attributes(all attributes, even psychological attributes)\n",
    "Please follow the above processes for dividing time points and determining key attributes.\n",
    "'''\n",
    "\n",
    "    # Build the prompt for the first API call\n",
    "    prompt1 = f\"\"\"{instruction}\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "    \n",
    "    try:\n",
    "        # First API call\n",
    "        response1 = model.generate_content(prompt1)\n",
    "        usage1 = response1.usage_metadata\n",
    "        input_tokens1 = usage1.prompt_token_count\n",
    "        # print(f\"Request: future_time_sequence_planner (first call), Tokens used - Input: {input_tokens1}, Output: {usage1.candidates_token_count}, Total: {usage1.total_token_count}\")\n",
    "        \n",
    "        # Extract the generated text\n",
    "        generated_text = response1._result.candidates[0].content.parts[0].text\n",
    "\n",
    "        # Build prompt2 for refining the analysis\n",
    "        prompt2 = f\"\"\"Prompt: {prompt1}\n",
    "\n",
    "Response: {generated_text}\n",
    "_________________________________\n",
    "Please analyze the above Response, confirm if the social and psychological attributes are correctly classified, and refine them accordingly. Ensure:\n",
    "1. Social attributes focus on measurable, tangible aspects of society (e.g., infrastructure, environment, health metrics).\n",
    "2. Psychological attributes focus on group psychological activities, such as perceptions, attitudes, and opinions.\n",
    "Do not add any word like \" (measured through surveys) \" in psychological attributes, ppl know how to get these attributes\n",
    "After refining, ensure each list contains exactly 20 attributes.\"\"\"\n",
    "\n",
    "        # Second API call with prompt2\n",
    "        response2 = model.generate_content(prompt2)\n",
    "        usage2 = response2.usage_metadata\n",
    "        input_tokens2 = usage2.prompt_token_count\n",
    "        # print(f\"Request: future_time_sequence_planner (second call), Tokens used - Input: {input_tokens2}, Output: {usage2.candidates_token_count}, Total: {usage2.total_token_count}\")\n",
    "\n",
    "        # Extract the refined analysis\n",
    "        refined_text = response2._result.candidates[0].content.parts[0].text\n",
    "\n",
    "        # Updated prompt3\n",
    "        prompt3 = f\"\"\"{instruction}\n",
    "___________________________\n",
    "{response1}\n",
    "{refined_text}\n",
    "_____________________\n",
    "Please extract the time points and the two sets of attributes from the last refined content, and output them as a JSON object with three fields:\n",
    "- \"time_points\": a list of strings representing the time points.\n",
    "- \"physical_questions\": a list of exactly 20 strings, where each string is an individual social attribute.\n",
    "- \"psychological_questions\": a list of exactly 20 strings, where each string is an individual group psychological attribute.\n",
    "\n",
    "Ensure that the time points and attributes are correctly extracted.\n",
    "- **Use simple present tense** without emphasizing any specific time points.\n",
    "- **Use straightforward language**.\n",
    "\"\"\"\n",
    "\n",
    "        # Define the valid JSON Schema for the expected response\n",
    "        response_schema = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"time_points\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"physical_questions\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"psychological_questions\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"time_points\", \"physical_questions\", \"psychological_questions\"]\n",
    "        }\n",
    "\n",
    "        # Third API call, requesting structured output with the correct schema\n",
    "        response3 = model.generate_content(\n",
    "            prompt3,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=response_schema\n",
    "            ),\n",
    "        )\n",
    "        usage3 = response3.usage_metadata\n",
    "        input_tokens3 = usage3.prompt_token_count\n",
    "        # print(f\"Request: future_time_sequence_planner (third call), Tokens used - Input: {input_tokens3}, Output: {usage3.candidates_token_count}, Total: {usage3.total_token_count}\")\n",
    "\n",
    "        # Extract the time points and questions from response3\n",
    "        data = json.loads(response3._result.candidates[0].content.parts[0].text)\n",
    "\n",
    "        # Extract time points and questions\n",
    "        time_points = data.get('time_points', [])\n",
    "        physical_questions = data.get('physical_questions', [])[:20]\n",
    "        psychological_questions = data.get('psychological_questions', [])[:20]\n",
    "\n",
    "        # Note: The variables 'physical_questions' and 'psychological_questions' now contain attribute names as per the updated requirements.\n",
    "\n",
    "        # Create the 'prompttemp' directory if it doesn't exist\n",
    "        prompttemp_dir = os.path.join(\"prompttemp\")\n",
    "        os.makedirs(prompttemp_dir, exist_ok=True)\n",
    "\n",
    "        # Save prompt1 and response1\n",
    "        with open(os.path.join(prompttemp_dir, 'future_time_sequence_planner_prompt1.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== Prompt 1 ===\\n\")\n",
    "            f.write(prompt1)\n",
    "            f.write(\"\\n\\n=== Response 1 ===\\n\")\n",
    "            f.write(response1._result.candidates[0].content.parts[0].text)\n",
    "\n",
    "        # Save prompt2 and response2\n",
    "        with open(os.path.join(prompttemp_dir, 'future_time_sequence_planner_prompt2.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== Prompt 2 ===\\n\")\n",
    "            f.write(prompt2)\n",
    "            f.write(\"\\n\\n=== Response 2 ===\\n\")\n",
    "            f.write(response2._result.candidates[0].content.parts[0].text)\n",
    "\n",
    "        # Save prompt3 and response3\n",
    "        with open(os.path.join(prompttemp_dir, 'future_time_sequence_planner_prompt3.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== Prompt 3 ===\\n\")\n",
    "            f.write(prompt3)\n",
    "            f.write(\"\\n\\n=== Response 3 ===\\n\")\n",
    "            f.write(response3._result.candidates[0].content.parts[0].text)\n",
    "\n",
    "        return time_points, physical_questions, psychological_questions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return [], [], []\n",
    "    \n",
    "\n",
    "def answer_questions(post_string, questions, include_questions_in_output=True, usage_log_file='usage_log.json'):\n",
    "    \"\"\"\n",
    "    Analyzes a given post to determine its impact on a list of attributes.\n",
    "\n",
    "    This function processes the post content using an LLM to extract insights related to the specified questions.\n",
    "    It generates an analysis that highlights how the post influences each attribute.\n",
    "\n",
    "    Parameters:\n",
    "    - post_string (str): The content of the post to analyze.\n",
    "    - questions (List[str]): A list of attribute questions to address.\n",
    "    - include_questions_in_output (bool, optional): Whether to include the questions in the output. Defaults to True.\n",
    "    - usage_log_file (str, optional): Path to the usage log file. Defaults to 'usage_log.json'.\n",
    "\n",
    "    Returns:\n",
    "    - result_post (str): The analysis result as a string.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    # Combine all questions into a single string\n",
    "    questions_text = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\n",
    "\n",
    "    # Updated prompt to meet the new requirements\n",
    "    prompt = f\"\"\"\n",
    "You are an analyst tasked with analyzing and summarizing the current attributes of human society.\n",
    "You have received the following piece of information. Analyze how it impacts any of the listed attributes and explain the specific impact.\n",
    "    •   If the information is a news article, focus on whether it provides quantitative analysis for any attributes and include those details.\n",
    "    •   If it comes from sources like Reddit or similar discussions, determine if there are qualitative trends and describe the observed trends.\n",
    "    •   Sumary the information with in one sentence first, then provide your analysis directly without reviewing the article.\n",
    "    •   Avoid including any disclaimers or additional notes. Write with confidence, as others will judge the accuracy of your analysis.\n",
    "Use attribute names to indicate which attribute you are referring to, and avoid using any numbering. Provide concise, valuable insights, and do not mention attributes you cannot analyze.\n",
    "——————————————————————\n",
    "Post Content:\n",
    "{post_string}\n",
    "——————————————————————\n",
    "Questions:\n",
    "{questions_text}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # print(\"Making API request to 'answer_questions'...\")\n",
    "\n",
    "        # Generate response\n",
    "        response = model.generate_content(prompt)\n",
    "        usage = response.usage_metadata\n",
    "        input_tokens = usage.prompt_token_count\n",
    "        # print(f\"API call to 'answer_questions' completed. Tokens used - Input: {input_tokens}, Output: {usage.candidates_token_count}, Total: {usage.total_token_count}\")\n",
    "        answer = response._result.candidates[0].content.parts[0].text.strip()\n",
    "\n",
    "        # Save the prompt and response to 'prompttemp/answer_questions_prompts.txt'\n",
    "        prompttemp_dir = os.path.join(\"prompttemp\")\n",
    "        os.makedirs(prompttemp_dir, exist_ok=True)\n",
    "        prompt_file = os.path.join(prompttemp_dir, 'answer_questions_prompts.txt')\n",
    "\n",
    "        with open(prompt_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(\"=== Prompt ===\\n\")\n",
    "            f.write(prompt)\n",
    "            f.write(\"\\n\\n=== Response ===\\n\")\n",
    "            f.write(answer)\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "        # Log usage data (non-caching request)\n",
    "        log_usage_data(\n",
    "            op_name='extract_individual_contributions',\n",
    "            model_name=model.model_name,\n",
    "            prompt_tokens=input_tokens,\n",
    "            candidates_tokens=usage.candidates_token_count,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        answer = 'Error generating answer'\n",
    "\n",
    "    # Build the output text based on the flag\n",
    "    if include_questions_in_output:\n",
    "        qa_text = answer\n",
    "    else:\n",
    "        qa_text = answer\n",
    "\n",
    "    result_post = f\"{qa_text}\"\n",
    "\n",
    "    return result_post\n",
    "\n",
    "\n",
    "class CachedDeriveOverallState:\n",
    "    \"\"\"\n",
    "    A class that leverages cached content to generate summaries of the overall societal state.\n",
    "\n",
    "    This class initializes a cache containing a large prompt (`content_string`) to reduce redundant API calls\n",
    "    when generating summaries based on different question sets. It uses Google's Generative AI models with caching\n",
    "    capabilities to efficiently produce summaries.\n",
    "\n",
    "    Parameters:\n",
    "    - content_string (str): The long text containing numerous pieces of collected information about society.\n",
    "    - model_name (str, optional): The name of the model to use. Defaults to 'models/gemini-1.5-flash-001'.\n",
    "    - usage_log_file (str, optional): Path to the usage log file. Defaults to 'usage_log.json'.\n",
    "\n",
    "    Methods:\n",
    "    - generate_summary(questions, usage_log_file='usage_log.json'): Generates a summary focusing on the provided social attributes.\n",
    "    - delete_cache(): Deletes the cached content to free up resources.\n",
    "    \"\"\"\n",
    "    def __init__(self, content_string, model_name='models/gemini-1.5-flash-001', usage_log_file='usage_log.json'):\n",
    "\n",
    "        self.content_string = content_string\n",
    "\n",
    "        # Prepare the prompt up to the point to be cached\n",
    "        cached_prompt = f\"\"\"\n",
    "You are a sociologist, and below is a long text, followed by a series of social attributes you have been recently studying.\n",
    "The long text contains numerous pieces of collected information about society, each ending with a brief analysis that highlights observations related to the social attributes you are studying.\n",
    "Based on the analysis of all the information in the long text, write a summary about the current state of society, focusing on the social attributes listed below. The summary should describe the overall state of society.\n",
    "If you cannot summarize the quantifiable physical attributes you are studying from the long text, then provide reasonable values based on your real-world experience.\n",
    "————————————————————————————————————————————————————\n",
    "Long Text:\n",
    "{self.content_string}\n",
    "————————————————————————————————————————————————————\n",
    "\"\"\"\n",
    "\n",
    "        # Use the uploaded file to create cached content\n",
    "        self.cached_content = caching.CachedContent.create(\n",
    "            model='models/gemini-1.5-flash-001',\n",
    "            display_name=model_name,  # used to identify the cache\n",
    "            contents=[cached_prompt],\n",
    "            ttl=timedelta(minutes=5),\n",
    "        )\n",
    "        # Create the model using the cached content\n",
    "        self.model = genai.GenerativeModel.from_cached_content(cached_content=self.cached_content)\n",
    "\n",
    "        # After creating CachedContent\n",
    "        cache_prompt_tokens = self.cached_content.usage_metadata.total_token_count\n",
    "\n",
    "        # Log caching storage usage\n",
    "        log_usage_data(\n",
    "            op_name='derive_overall_state_cache_storage',\n",
    "            model_name=model_name,\n",
    "            prompt_tokens=0,\n",
    "            candidates_tokens=0,\n",
    "            cached_content_tokens=cache_prompt_tokens,\n",
    "            duration_seconds=600,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "\n",
    "    def generate_summary(self, questions, usage_log_file='usage_log.json'):\n",
    "        # Build the prompt with the variable part\n",
    "        prompt = f\"\"\"\n",
    "The Social Attributes You Are Studying:\n",
    "{'\\n'.join(questions)}\n",
    "————————————————————————————————————————————————————\n",
    "Summary of Current State:\"\"\"\n",
    "\n",
    "        # Generate the response\n",
    "        response = self.model.generate_content(prompt)\n",
    "        summary = response._result.candidates[0].content.parts[0].text.strip()\n",
    "        \n",
    "        # After generating the response\n",
    "        usage = response.usage_metadata\n",
    "\n",
    "        # Log usage data\n",
    "        log_usage_data(\n",
    "            op_name='derive_overall_state_from_cache',\n",
    "            model_name=self.model.model_name,\n",
    "            prompt_tokens=usage.prompt_token_count,\n",
    "            candidates_tokens=usage.candidates_token_count,\n",
    "            cached_content_tokens=usage.cached_content_token_count,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "        print(f\"[longcontext] Tokens used - Prompt: {usage.prompt_token_count}, Candidates: {usage.candidates_token_count}, Cached Content: {usage.cached_content_token_count}\")\n",
    "        return summary\n",
    "\n",
    "    def delete_cache(self):\n",
    "        # Delete the cached content\n",
    "        self.cached_content.delete()\n",
    "\n",
    "def derive_overall_state(content_string, questions, usage_log_file='usage_log.json'):\n",
    "    \"\"\"\n",
    "    Generates a summary of the overall societal state based on provided content and attributes.\n",
    "\n",
    "    This function accepts a long string containing various news articles, Reddit posts, and other\n",
    "    pieces of information about society. It uses a language model to analyze this content and\n",
    "    produce a concise summary focusing on the specified societal attributes.\n",
    "\n",
    "    Parameters:\n",
    "    - content_string (str): The long text containing the collected societal information to be summarized.\n",
    "    - questions (list of str): A list of societal attributes (questions) to focus on in the summary.\n",
    "    - usage_log_file (str, optional): The path to the usage log file for logging API usage.\n",
    "      Defaults to 'usage_log.json'.\n",
    "\n",
    "    Returns:\n",
    "    - summary (str): A string containing the summary of the current societal state,\n",
    "      focusing on the provided attributes. The summary starts with \"State: \".\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine all questions into a single string\n",
    "    questions_text = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a sociologist. Below is a long text containing numerous pieces of information about society, each accompanied by a brief analysis. These analyses correspond to certain attributes listed below:\n",
    "\n",
    "————————————————————————————————————————————————————\n",
    "The societal attributes you are studying:\n",
    "{questions_text}\n",
    "————————————————————————————————————————————————————\n",
    "Long text:\n",
    "{content_string}\n",
    "————————————————————————————————————————————————————\n",
    "\n",
    "Based on the analyses of all the provided information, write a summary of the current state of society, focusing on the societal attributes you are studying.\n",
    "\n",
    "The summary must confidently describe the current societal condition, avoiding any speculative language.\n",
    "\n",
    "Please note:\n",
    "    1. This summary is intended for readers to understand the current state of society. Therefore, do not include your analyses in the summary. It must solely describe societal conditions.\n",
    "    2. Do not add any additional “Note:” at the end. Only provide the description of the societal state you are studying.\n",
    "    3. If you cannot summarize the quantifiable physical attributes you are studying from the long text, then provide reasonable values based on your real-world experience.\n",
    "    4. Start your summary with “State: “.\n",
    "\n",
    "State: \n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "        # Generate the summary\n",
    "        response = model.generate_content(prompt)\n",
    "        summary = response._result.candidates[0].content.parts[0].text.strip()\n",
    "\n",
    "        # Print token usage\n",
    "        usage = response.usage_metadata\n",
    "        # print(f\"API call to 'derive_overall_state' completed. Tokens used - Input: {usage.prompt_token_count}, Output: {usage.candidates_token_count}, Total: {usage.total_token_count}\")\n",
    "\n",
    "        # Log usage data\n",
    "        log_usage_data(\n",
    "            op_name='derive_overall_state',\n",
    "            model_name=model.model_name,\n",
    "            prompt_tokens=usage.prompt_token_count,\n",
    "            candidates_tokens=usage.candidates_token_count,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "        print(f\"[longcontext] Tokens used - Prompt: {usage.prompt_token_count}, \"\n",
    "              f\"Candidates: {usage.candidates_token_count}\")\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def predict_individual_future_actions(\n",
    "    physical_state,\n",
    "    current_time_point,\n",
    "    post,\n",
    "    psychological_state,\n",
    "    future_time_point,\n",
    "    physical_attributes,\n",
    "    usage_log_file='usage_log.json'\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulates an individual's future actions and their impact on observed attributes.\n",
    "\n",
    "    Parameters:\n",
    "    - physical_state: String representing the current physical overall state.\n",
    "    - current_time_point: String representing the current time point.\n",
    "    - post: String representing the individual post/information.\n",
    "    - psychological_state: String representing the current psychological state.\n",
    "    - future_time_point: String representing the future time point.\n",
    "    - physical_attributes: List of physical attributes we are observing.\n",
    "    - usage_log_file: String representing the file path where usage logs will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - impact_data: Dictionary containing the individual's actions and impact analysis.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an ordinary member of society, currently at the time point {current_time_point}, and you have just come across the following piece of information. Based on the societal state and people’s psychological condition, simulate a reasonable reaction and the actions you would take.\n",
    "Then analyze how your actions would influence the societal attributes you are researching at the future time point {future_time_point}.\n",
    "\n",
    "Societal attributes you are researching:\n",
    "{', '.join(physical_attributes)}\n",
    "\n",
    "The information you just came across:\n",
    "{post}\n",
    "\n",
    "The societal state at this time:\n",
    "{physical_state}\n",
    "\n",
    "The psychological state of people at this time:\n",
    "{psychological_state}\n",
    "\n",
    "Please first write down your possible reactions and actions based on people’s psychological state at this moment.\n",
    "Then describe how your actions would affect the societal attributes you are researching at the future time point.\n",
    "\n",
    "Important Notes:\n",
    "    1.  Individual impact is minimal and well understood. If you believe your actions have no impact on a specific attribute, do not mention that attribute at all.\n",
    "    2.  If any attributes are influenced, no matter how small, describe the trend of the impact.\n",
    "    3.  Stay fully immersed in this role. Do not break character. Every word should reflect this role.\n",
    "    4.  Use concise language. Only focus on key points.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Generate the response with the new generation configuration\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=1.5\n",
    "            )\n",
    "        )\n",
    "        generated_text = response._result.candidates[0].content.parts[0].text.strip()\n",
    "\n",
    "        # Get usage data\n",
    "        usage = response.usage_metadata\n",
    "        input_tokens = usage.prompt_token_count\n",
    "\n",
    "        # Log usage data (non-caching request)\n",
    "        log_usage_data(\n",
    "            op_name='predict_individual_future_actions',\n",
    "            model_name=model.model_name,\n",
    "            prompt_tokens=input_tokens,\n",
    "            candidates_tokens=usage.candidates_token_count,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "\n",
    "        # Ensure prompttemp directory exists\n",
    "        prompttemp_dir = os.path.join(\"prompttemp\")\n",
    "        os.makedirs(prompttemp_dir, exist_ok=True)\n",
    "\n",
    "        # Append prompt and response to the same text file\n",
    "        with open(os.path.join(prompttemp_dir, 'predict_individual_future_actions_prompts.txt'), 'a', encoding='utf-8') as f:\n",
    "            f.write(\"=== Prompt ===\\n\")\n",
    "            f.write(prompt)\n",
    "            f.write(\"\\n\\n=== Response ===\\n\")\n",
    "            f.write(generated_text)\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "        # Return the generated text as a string\n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "def forecast_overall_future_state(content_string, physical_attributes, future_time_point, current_time_point, usage_log_file='usage_log.json'):\n",
    "    \"\"\"\n",
    "    Predicts the future overall state based on individual future actions.\n",
    "\n",
    "    Parameters:\n",
    "    - content_string: A long string containing individual future actions.\n",
    "    - physical_attributes: List of future social attributes.\n",
    "    - future_time_point: The future time point.\n",
    "    - current_time_point: The current time point.\n",
    "    - usage_log_file: Path to the file where usage logs will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - A string containing the predicted future overall state.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the prompt as per the new requirements\n",
    "    prompt = f\"\"\"\n",
    "You are a sociologist currently situated at {current_time_point}. You have gathered a list of information.\n",
    "Each piece of information details a human activity and its impact on the future time point {future_time_point}.\n",
    "—————————————————————————\n",
    "The information you have collected:\n",
    "{content_string}\n",
    "—————————————————————————\n",
    "You are particularly focused on the following social attributes at the future time point {future_time_point}:\n",
    "{', '.join(physical_attributes)}\n",
    "—————————————————————————\n",
    "Based on the information you have collected, predict the status of the social attributes you are focusing on at the future time point.\n",
    "\n",
    "Please note:\n",
    "    1. Use a confident and assertive tone to describe the future state.\n",
    "    2. Write the final results, do not include the analytical process in your response.\n",
    "    3. If the basis for predicting a certain physical attribute is insufficient, please still provide your predicted value based on your experience, as this is very important.\n",
    "    4. Your prediction should be a concrete, quantified numerical attribute rather than a trend of change in the attribute.”\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        usage = response.usage_metadata\n",
    "        input_tokens = usage.prompt_token_count\n",
    "        # print(f\"API call to 'forecast_overall_future_state' completed. Tokens used - Input: {input_tokens}, Output: {usage.candidates_token_count}, Total: {usage.total_token_count}\")\n",
    "        prediction = response._result.candidates[0].content.parts[0].text.strip()\n",
    "\n",
    "        # Ensure prompttemp directory exists\n",
    "        prompttemp_dir = os.path.join(\"prompttemp\")\n",
    "        os.makedirs(prompttemp_dir, exist_ok=True)\n",
    "\n",
    "        # Save prompt and response to the same file\n",
    "        with open(os.path.join(prompttemp_dir, 'forecast_overall_future_state_prompts.txt'), 'a', encoding='utf-8') as f:\n",
    "            f.write(\"=== Prompt ===\\n\")\n",
    "            f.write(prompt)\n",
    "            f.write(\"\\n\\n=== Response ===\\n\")\n",
    "            f.write(prediction)\n",
    "            f.write(\"\\n\\n\")\n",
    "        \n",
    "        # Log usage data\n",
    "        log_usage_data(\n",
    "            op_name='forecast_overall_future_state',\n",
    "            model_name=model.model_name,\n",
    "            prompt_tokens=usage.prompt_token_count,\n",
    "            candidates_tokens=usage.candidates_token_count,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "        print(f\"[longcontext] Tokens used - Prompt: {usage.prompt_token_count}, \"\n",
    "              f\"Candidates: {usage.candidates_token_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        prediction = 'Error generating prediction'\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def simulate_future_individual_behavior(physical_state, current_time_point, post_string, usage_log_file='usage_log.json'):\n",
    "    \"\"\"\n",
    "    Simulates what the author of a given post/news might say or write in the current world state.\n",
    "\n",
    "    Parameters:\n",
    "    - physical_state: String describing the current state of the world.\n",
    "    - current_time_point: String representing the current time point.\n",
    "    - post_string: String containing the original post or news article.\n",
    "    - usage_log_file: String representing the path to the usage log file.\n",
    "\n",
    "    Returns:\n",
    "    - A string containing the simulated behavior of the same author in the current time.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a journalist and a frequent Reddit user. The current time is {current_time_point}.\n",
    "\n",
    "In the past, you posted the following news article or Reddit post. Please write a news article from the same institution or a Reddit post that you would participate in, based on your personality and values and the current state of the world.\n",
    "Note:\n",
    "1） If you posted a news article in the past, write a news article now. If you posted a Reddit post in the past, write a Reddit post now.\n",
    "The current state of the world is:\n",
    "{physical_state}.\n",
    "_______________________\n",
    "News article or Reddit post you posted in the past:\n",
    "{post_string}\n",
    "_______________________\n",
    "The news article or Reddit post you are writing now:\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        usage = response.usage_metadata\n",
    "        input_tokens = usage.prompt_token_count\n",
    "        # print(f\"API call to 'simulate_future_individual_behavior' completed. Tokens used - Input: {input_tokens}, Output: {usage.candidates_token_count}, Total: {usage.total_token_count}\")\n",
    "        simulation = response._result.candidates[0].content.parts[0].text.strip()\n",
    "\n",
    "        # Ensure prompttemp directory exists\n",
    "        prompttemp_dir = os.path.join(\"prompttemp\")\n",
    "        os.makedirs(prompttemp_dir, exist_ok=True)\n",
    "\n",
    "        # Save prompt and response\n",
    "        with open(os.path.join(prompttemp_dir, 'simulate_future_individual_behavior_prompts.txt'), 'a', encoding='utf-8') as f:\n",
    "            f.write(\"=== Prompt ===\\n\")\n",
    "            f.write(prompt)\n",
    "            f.write(\"\\n\\n=== Response ===\\n\")\n",
    "            f.write(simulation)\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "        # Log usage data\n",
    "        log_usage_data(\n",
    "            op_name='simulate_future_individual_behavior',\n",
    "            model_name=model.model_name,\n",
    "            prompt_tokens=input_tokens,\n",
    "            candidates_tokens=usage.candidates_token_count,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        simulation = 'Error generating simulation'\n",
    "\n",
    "    return simulation\n",
    "\n",
    "def summarize_prediction(question, world_state, current_time, usage_log_file='usage_log.json'):\n",
    "    \"\"\"\n",
    "    Answers a question based on the current world state and time.\n",
    "\n",
    "    Parameters:\n",
    "    - question: The question to be answered.\n",
    "    - world_state: The current world state description.\n",
    "    - current_time: The current time point.\n",
    "    - usage_log_file: The file path to save the usage log.\n",
    "\n",
    "    Returns:\n",
    "    - answer: The answer to the question.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a sociologist in the year {current_time}. Imagine that you are sitting in a long-abandoned library, untouched for decades. \n",
    "Among the dust-covered books and old records, you discover a question written on a piece of paper left behind by someone from the past.\n",
    "\n",
    "Based on the current world state, please answer the question confidently and decisively, without any hesitation. Imagine the person who wrote the question hoped that someone like you would answer it one day. Please ensure your answer is clear, concise, and reflects the present state of the world.\n",
    "\n",
    "World State:\n",
    "{world_state}\n",
    "\n",
    "Question found in the abandoned library:\n",
    "{question}\n",
    "\n",
    "Your Answer:\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.7\n",
    "            )\n",
    "        )\n",
    "        usage = response.usage_metadata\n",
    "        input_tokens = usage.prompt_token_count\n",
    "        print(f\"API call to 'summarize_prediction' completed. Tokens used - Input: {input_tokens}, Output: {usage.candidates_token_count}, Total: {usage.total_token_count}\")\n",
    "        answer = response._result.candidates[0].content.parts[0].text.strip()\n",
    "\n",
    "        # Ensure 'prompttemp' directory exists\n",
    "        prompttemp_dir = os.path.join(\"prompttemp\")\n",
    "        os.makedirs(prompttemp_dir, exist_ok=True)\n",
    "\n",
    "        # Save prompt and response\n",
    "        with open(os.path.join(prompttemp_dir, 'summarize_prediction_prompts.txt'), 'a', encoding='utf-8') as f:\n",
    "            f.write(\"=== Prompt ===\\n\")\n",
    "            f.write(prompt)\n",
    "            f.write(\"\\n\\n=== Response ===\\n\")\n",
    "            f.write(answer)\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "        # Log usage data (non-caching request)\n",
    "        log_usage_data(\n",
    "            op_name='summarize_prediction',\n",
    "            model_name=model.model_name,\n",
    "            prompt_tokens=input_tokens,\n",
    "            candidates_tokens=usage.candidates_token_count,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        answer = 'Error generating answer'\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb0102e-992b-4d07-8356-1cc9e08c8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_save_individual_contributions(input_paths, questions, output_file, usage_log_file='usage_log.json'):\n",
    "    \"\"\"\n",
    "    Reads posts from input paths, processes each post by calling the 'answer_questions' function in parallel,\n",
    "    and writes the processed contributions to the output file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_paths: List of file paths or directories to read posts from.\n",
    "    - questions: List of questions to process each post.\n",
    "    - output_file: Path to the output file where processed contributions will be saved.\n",
    "    - usage_log_file: Path to the log file where usage data will be saved.\n",
    "    \"\"\"\n",
    "    posts = read_posts_file(input_paths)\n",
    "    total_posts = len(posts)\n",
    "    posts = posts[:2000]\n",
    "    def process_post(idx_post):\n",
    "        idx, post = idx_post\n",
    "        # print(f\"Processing post {idx}/{total_posts} in Step 1\")\n",
    "        if isinstance(post, dict):\n",
    "            post_string = \"\\n\".join(f\"{key}: {value}\" for key, value in post.items())\n",
    "        else:\n",
    "            post_string = str(post)\n",
    "        result_post = answer_questions(post_string, questions, usage_log_file=usage_log_file)\n",
    "        return result_post\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        processed_posts = list(executor.map(process_post, enumerate(posts, start=1)))\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_posts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # print(f\"Processed contributions saved to {output_file}\")\n",
    "\n",
    "def process_overall_state(\n",
    "    input_data,\n",
    "    questions,\n",
    "    output_file=None,\n",
    "    usage_log_file='usage_log.json'\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes the overall state by summarizing individual contributions and generating a summary.\n",
    "\n",
    "    This function reads input data (which can be a file path or data), selects posts based on a token limit,\n",
    "    concatenates them into a single string, and then uses the `derive_overall_state` function to generate\n",
    "    a summary of the current state focusing on the provided questions.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data: Path to the input file or the actual data to be processed.\n",
    "    - questions: List of strings representing the questions or attributes to focus on in the summary.\n",
    "    - output_file (str, optional): If provided, the summary will be saved to this file path.\n",
    "    - usage_log_file (str, optional): Path to the usage log file for logging API usage. Defaults to 'usage_log.json'.\n",
    "\n",
    "    Returns:\n",
    "    - summary (str): A string containing the summary of the current state.\n",
    "    \"\"\"\n",
    "    target_token_limit = 1000000\n",
    "    selected_posts = select_posts_by_token_limit(input_data, target_token_limit, model=\"gemini-1.5-flash\")\n",
    "    content_string = \"\\n\\n\".join(selected_posts)\n",
    "    summary = derive_overall_state(content_string, questions, usage_log_file=usage_log_file)\n",
    "\n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_current_state(contributions_text, physical_questions, psychological_questions, physical_output_file, psychological_output_file, usage_log_file='usage_log.json'):\n",
    "    \"\"\"\n",
    "    Summarizes the current physical and psychological state based on individual contributions.\n",
    "\n",
    "    This function takes the processed individual contributions, creates a cached state for efficient processing,\n",
    "    and generates summaries for both physical and psychological attributes by invoking `derive_overall_state`.\n",
    "    The summaries are saved to the specified output files.\n",
    "\n",
    "    Parameters:\n",
    "    - contributions_text: List of strings containing the processed individual contributions.\n",
    "    - physical_questions: List of strings representing the physical attributes to focus on.\n",
    "    - psychological_questions: List of strings representing the psychological attributes to focus on.\n",
    "    - physical_output_file (str): Path to the file where the physical state summary will be saved.\n",
    "    - psychological_output_file (str): Path to the file where the psychological state summary will be saved.\n",
    "    - usage_log_file (str, optional): Path to the usage log file for logging API usage. Defaults to 'usage_log.json'.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    contributions_text = '\\n\\n\\n\\n'.join(contributions_text)\n",
    "    cached_state = CachedDeriveOverallState(contributions_text, usage_log_file=usage_log_file)\n",
    "\n",
    "    # print(\"Starting derive_overall_state with caching for physical_questions...\")\n",
    "    physical_state_summary = cached_state.generate_summary(physical_questions, usage_log_file=usage_log_file)\n",
    "    with open(physical_output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(physical_state_summary, f, ensure_ascii=False, indent=4)\n",
    "    # print(f\"Physical overall state saved to {physical_output_file}\")\n",
    "\n",
    "    # print(\"Starting derive_overall_state with caching for psychological_questions...\")\n",
    "    psychological_state_summary = cached_state.generate_summary(psychological_questions, usage_log_file=usage_log_file)\n",
    "    with open(psychological_output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(psychological_state_summary, f, ensure_ascii=False, indent=4)\n",
    "    # print(f\"Psychological overall state saved to {psychological_output_file}\")\n",
    "\n",
    "    cached_state.delete_cache()\n",
    "\n",
    "\n",
    "def process_and_save_individual_future_actions(\n",
    "    physical_state_file,\n",
    "    psychological_state_file,\n",
    "    current_time_point,\n",
    "    posts_file,\n",
    "    future_time_point,\n",
    "    physical_attributes,\n",
    "    output_file,\n",
    "    usage_log_file='usage_log.json'\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads inputs from files, processes individual future actions in parallel, and writes the output to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - physical_state_file: Path to the physical overall state JSON file.\n",
    "    - psychological_state_file: Path to the psychological overall state JSON file.\n",
    "    - current_time_point: The current time point as a string.\n",
    "    - posts_file: Path to the JSON file containing posts.\n",
    "    - future_time_point: The future time point as a string.\n",
    "    - physical_attributes: List of physical attributes we are observing.\n",
    "    - output_file: Path to the output JSON file.\n",
    "    - usage_log_file: Path to the usage log file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read physical state\n",
    "    with open(physical_state_file, 'r', encoding='utf-8') as f:\n",
    "        physical_state = json.load(f)\n",
    "\n",
    "    # Read psychological state\n",
    "    with open(psychological_state_file, 'r', encoding='utf-8') as f:\n",
    "        psychological_state = json.load(f)\n",
    "\n",
    "    # Read posts\n",
    "    with open(posts_file, 'r', encoding='utf-8') as f:\n",
    "        posts = json.load(f)\n",
    "    posts = posts[:2000]\n",
    "    total_posts = len(posts)\n",
    "\n",
    "    def process_post(idx_post):\n",
    "        idx, post = idx_post\n",
    "        # print(f\"Processing post {idx}/{total_posts} in Step 3\")\n",
    "        if isinstance(post, dict):\n",
    "            post_string = \"\\n\".join(f\"{key}: {value}\" for key, value in post.items())\n",
    "        else:\n",
    "            post_string = str(post)\n",
    "        result_post = predict_individual_future_actions(\n",
    "            physical_state,\n",
    "            current_time_point,\n",
    "            post_string,\n",
    "            psychological_state,\n",
    "            future_time_point,\n",
    "            physical_attributes,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "        return result_post\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        processed_posts = list(executor.map(process_post, enumerate(posts, start=1)))\n",
    "\n",
    "    # Save the results\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_posts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # print(f\"Processed future actions saved to {output_file}\")\n",
    "\n",
    "def process_forecast_overall_future_state_with_io(\n",
    "    input_file,\n",
    "    physical_attributes,\n",
    "    future_time_point,\n",
    "    current_time_point,\n",
    "    output_file,\n",
    "    usage_log_file='usage_log.json'\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads individual future actions from an input file, processes them, and writes the future overall state to the output file.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file: Path to the input file containing individual future actions.\n",
    "    - physical_attributes: List of social attributes to focus on.\n",
    "    - future_time_point: The future time point.\n",
    "    - current_time_point: The current time point.\n",
    "    - output_file: Path to the output file where the future overall state will be saved.\n",
    "    - usage_log_file: Path to the file where usage logs will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the token selection utility to filter posts\n",
    "    target_token_limit = 1000000  # Set a target token limit slightly below 1 million\n",
    "    selected_posts = select_posts_by_token_limit(input_file, target_token_limit, model=\"gemini-1.5-flash\")\n",
    "\n",
    "    # Concatenate selected posts into a single string with separators\n",
    "    content_string = '\\n\\n'.join(selected_posts)\n",
    "\n",
    "    # Call forecast_overall_future_state\n",
    "    # print(\"Starting forecast_overall_future_state...\")\n",
    "    start_time = time.time()\n",
    "    future_state = forecast_overall_future_state(\n",
    "        content_string,\n",
    "        physical_attributes,\n",
    "        future_time_point,\n",
    "        current_time_point,\n",
    "        usage_log_file=usage_log_file\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    # print(f\"forecast_overall_future_state execution time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Write the future state to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(future_state, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def process_and_save_simulated_future_behavior(\n",
    "    physical_state_file,\n",
    "    current_time_point,\n",
    "    input_paths,\n",
    "    output_file,\n",
    "    usage_log_file='usage_log.json'\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads inputs from files, processes future individual behaviors in parallel, and writes the output to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - physical_state_file: Path to the physical overall state JSON file.\n",
    "    - current_time_point: The current time point as a string.\n",
    "    - input_paths: List of file paths or directories to read posts from.\n",
    "    - output_file: Path to the output JSON file.\n",
    "    - usage_log_file: Path to the usage log file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read physical state\n",
    "    with open(physical_state_file, 'r', encoding='utf-8') as f:\n",
    "        physical_state = json.load(f)\n",
    "\n",
    "    # Read posts\n",
    "    posts = read_posts_file(input_paths)\n",
    "    total_posts = len(posts)\n",
    "    posts = posts[:2000]\n",
    "    def process_post(idx_post):\n",
    "        idx, post = idx_post\n",
    "        # print(f\"Processing post {idx}/{total_posts} in Step 5\")\n",
    "        if isinstance(post, dict):\n",
    "            post_string = \"\\n\".join(f\"{key}: {value}\" for key, value in post.items())\n",
    "        else:\n",
    "            post_string = str(post)\n",
    "        result_post = simulate_future_individual_behavior(\n",
    "            physical_state,\n",
    "            current_time_point,\n",
    "            post_string,\n",
    "            usage_log_file=usage_log_file\n",
    "        )\n",
    "        return result_post\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        processed_posts = list(executor.map(process_post, enumerate(posts, start=1)))\n",
    "\n",
    "    # Save the results\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_posts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # print(f\"Simulated future posts saved to {output_file}\")\n",
    "\n",
    "def process_and_save_summarized_prediction(\n",
    "    question,\n",
    "    world_state_file,\n",
    "    current_time_point,\n",
    "    output_file,\n",
    "    usage_log_file='usage_log.json'\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads the world state from a file, processes the summary, and writes the answer to an output file.\n",
    "\n",
    "    Parameters:\n",
    "    - question: The user's question.\n",
    "    - world_state_file: Path to the JSON file containing the world state.\n",
    "    - current_time_point: The current time point as a string.\n",
    "    - output_file: Path to the output file where the summary will be saved.\n",
    "    - usage_log_file: Path to the usage log file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read world state\n",
    "    with open(world_state_file, 'r', encoding='utf-8') as f:\n",
    "        world_state = json.load(f)\n",
    "\n",
    "    # Process the summary\n",
    "    answer = summarize_prediction(\n",
    "        question=question,\n",
    "        world_state=world_state,\n",
    "        current_time=current_time_point,\n",
    "        usage_log_file=usage_log_file\n",
    "    )\n",
    "\n",
    "    # Save the answer to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(answer)\n",
    "\n",
    "    print(f\"Summary answer saved to {output_file}\")\n",
    "\n",
    "def process_filter_posts(post_strings, questions, num_processes=4):\n",
    "    \"\"\"\n",
    "    Filters a list of post strings, retaining only those posts related to the provided questions.\n",
    "\n",
    "    Parameters:\n",
    "    - post_strings (list of str): A list containing multiple post content strings.\n",
    "    - questions (list of str): A list of questions used to determine the relevance of each post.\n",
    "    - num_processes (int, optional): The number of parallel processes to use. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A filtered list of post strings that are relevant to the provided questions.\n",
    "\n",
    "    Functionality:\n",
    "    This function utilizes multiprocessing to filter the given list of post strings, retaining only those posts that are related to the provided list of questions.\n",
    "    \n",
    "    Steps:\n",
    "    1. Use the `partial` function to bind the `filter_post_by_questions` function with the fixed `questions` parameter.\n",
    "    2. Create a multiprocessing pool with a size of `num_processes`.\n",
    "    3. Apply the `filter_post_by_questions` function to each post string to determine its relevance.\n",
    "    4. Based on the results, filter and retain only the relevant posts.\n",
    "    \"\"\"\n",
    "    \n",
    "    partial_is_post_related = partial(filter_post_by_questions, questions=questions)\n",
    "\n",
    "    with Pool(num_processes) as pool:\n",
    "        results = pool.map(partial_is_post_related, post_strings)\n",
    "\n",
    "    filtered_posts = [post for post, is_related in zip(post_strings, results) if is_related]\n",
    "\n",
    "    num_filtered = len(filtered_posts)\n",
    "    num_original = len(post_strings)\n",
    "    # (f\"Filtered {num_filtered} posts out of {num_original} original posts.\")\n",
    "\n",
    "    return filtered_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287b6dda-3922-4912-9581-bf87c4a21622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psychohistory(data_dir, topic, crawl_data=True, filter_data=True, use_existing_sequence=False, reddit_limit=25, reddit_max_dialogues_per_comment=3, news_page_size=100):\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\n=== Step 1: Predicting future sequence ===\")\n",
    "    future_questions_file = os.path.join(data_dir, 'future_questions.json')\n",
    "\n",
    "    if use_existing_sequence:\n",
    "        # print(\"\\n=== Loading existing future sequence ===\")\n",
    "        if os.path.exists(future_questions_file):\n",
    "            with open(future_questions_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            time_points = data.get('time_points', [])\n",
    "            physical_questions = data.get('physical_questions', [])\n",
    "            psychological_questions = data.get('psychological_questions', [])\n",
    "            combined_questions = data.get('combined_questions', [])\n",
    "            # print(f\"Loaded future questions from {future_questions_file}\")\n",
    "        else:\n",
    "            print(f\"Error: {future_questions_file} does not exist. Cannot load existing future sequence.\")\n",
    "            return\n",
    "    else:\n",
    "        time_points, physical_questions, psychological_questions = predict_future_sequence(topic)\n",
    "        if not time_points:\n",
    "            print(\"Error: Failed to generate time points. Exiting the program.\")\n",
    "            return\n",
    "        combined_questions = physical_questions + psychological_questions\n",
    "        \n",
    "\n",
    "        with open(future_questions_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    'time_points': time_points,\n",
    "                    'physical_questions': physical_questions,\n",
    "                    'psychological_questions': psychological_questions,\n",
    "                    'combined_questions': combined_questions\n",
    "                },\n",
    "                f,\n",
    "                ensure_ascii=False,\n",
    "                indent=4\n",
    "            )\n",
    "        data = {\n",
    "            'time_points': time_points,\n",
    "            'physical_questions': physical_questions,\n",
    "            'psychological_questions': psychological_questions,\n",
    "            'combined_questions': combined_questions\n",
    "        }\n",
    "        # print(f\"Generated future questions and saved to {future_questions_file}\")\n",
    "    print(\"Time points:\", time_points)\n",
    "\n",
    "    if crawl_data:\n",
    "        print(\"\\n=== Step 2: Getting subreddits and news keywords ===\")\n",
    "        subreddits = get_reddit_subreddits_for_topic(topic, combined_questions)\n",
    "        subreddits = [subreddit.strip().lower().lstrip('r/').lstrip('/r/') for subreddit in subreddits]\n",
    "\n",
    "        news_keywords = get_news_keywords_for_topic(topic, combined_questions)\n",
    "\n",
    "        print(\"\\n=== Step 3: Crawling data ===\")\n",
    "        crawled_data_dir = os.path.join(data_dir, 'crawled_data')\n",
    "        os.makedirs(crawled_data_dir, exist_ok=True)\n",
    "\n",
    "        for subreddit in subreddits:\n",
    "            for sort_by in ['hot', 'new']:\n",
    "                try:\n",
    "                    get_subreddit_posts(\n",
    "                        keyword=subreddit,\n",
    "                        print_data=False,\n",
    "                        sort_by=sort_by,\n",
    "                        limit=reddit_limit,\n",
    "                        max_dialogues_per_comment=reddit_max_dialogues_per_comment,\n",
    "                        save_to_file=True,\n",
    "                        save_path=crawled_data_dir\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to download data from subreddit '{subreddit}' sorted by '{sort_by}'. Error: {e}\")\n",
    "\n",
    "        try:\n",
    "            get_news_articles(\n",
    "                keywords=news_keywords,\n",
    "                from_date=None,\n",
    "                to_date=None,\n",
    "                total_pages=1,\n",
    "                print_data=False,\n",
    "                page_size=news_page_size,\n",
    "                save_to_file=True,\n",
    "                save_path=crawled_data_dir\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download news articles. Error: {e}\")\n",
    "\n",
    "    print(\"\\n=== Step 4: Combining and selecting data ===\")\n",
    "    crawled_data_dir = os.path.join(data_dir, 'crawled_data')\n",
    "    data_files = [os.path.join(crawled_data_dir, f) for f in os.listdir(crawled_data_dir) if f.endswith('.json')]\n",
    "    \n",
    "    combined_posts_file = os.path.join(data_dir, 'combined_posts.json')\n",
    "    all_posts = read_posts_file(data_files)\n",
    "\n",
    "    with open(combined_posts_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_posts, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Combined posts saved to {combined_posts_file}\")\n",
    "    \n",
    "    if filter_data:\n",
    "        print(\"\\n=== Step 5: Filtering posts ===\")\n",
    "        filtered_posts = process_filter_posts(all_posts, combined_questions)\n",
    "\n",
    "        filtered_posts_file = os.path.join(data_dir, 'filtered_posts.json')\n",
    "        with open(filtered_posts_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(filtered_posts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # print(f\"Filtered posts saved to {filtered_posts_file}\")\n",
    "    else:\n",
    "        print(\"\\n=== Step 5: Loading combined posts from file ===\")\n",
    "        if os.path.exists(combined_posts_file):\n",
    "            with open(combined_posts_file, 'r', encoding='utf-8') as f:\n",
    "                all_posts = json.load(f)\n",
    "            # print(f\"Loaded combined posts from {combined_posts_file}\")\n",
    "        else:\n",
    "            print(f\"Error: {combined_posts_file} does not exist. Please run with filter_data=True first.\")\n",
    "            return\n",
    "\n",
    "\n",
    "    print(\"\\n=== Step 6: Extracting initial individual contributions ===\")\n",
    "    processed_contributions_file = os.path.join(data_dir, 'processed_contributions.json')\n",
    "    process_and_save_individual_contributions(\n",
    "        input_paths=[combined_posts_file],\n",
    "        questions=combined_questions,\n",
    "        output_file=processed_contributions_file\n",
    "    )\n",
    "    # print(f\"Processed contributions saved to {processed_contributions_file}\")\n",
    "\n",
    "    print(\"\\n=== Step 7: Deriving initial overall state ===\")\n",
    "    physical_questions = data.get('physical_questions', [])\n",
    "    psychological_questions = data.get('psychological_questions', [])\n",
    "\n",
    "    time_point_0_dir = os.path.join(data_dir, \"time_point_0\")\n",
    "    os.makedirs(time_point_0_dir, exist_ok=True)\n",
    "\n",
    "    physical_state_file = os.path.join(time_point_0_dir, 'physical_overall_state.json')\n",
    "    psychological_state_file = os.path.join(time_point_0_dir, 'psychological_overall_state.json')\n",
    "\n",
    "    with open(processed_contributions_file, 'r', encoding='utf-8') as f:\n",
    "        contributions_text = json.load(f)\n",
    "    summarize_current_state(\n",
    "        contributions_text=contributions_text,\n",
    "        physical_questions=physical_questions,\n",
    "        psychological_questions=psychological_questions,\n",
    "        physical_output_file=physical_state_file,\n",
    "        psychological_output_file=psychological_state_file,\n",
    "        usage_log_file='usage_log.json'\n",
    "    )\n",
    "    \n",
    "    # print(f\"Physical overall state saved to {physical_state_file}\")\n",
    "    # print(f\"Psychological overall state saved to {psychological_state_file}\")\n",
    "\n",
    "    input_posts_file = combined_posts_file  # Initial posts file\n",
    "    current_time_point = time_points[0]\n",
    "    num_time_points = len(time_points)\n",
    "\n",
    "    for i in range(1, num_time_points):\n",
    "        future_time_point = time_points[i]\n",
    "\n",
    "        print(f\"\\n=== Processing Time Point {i}/{num_time_points - 1} ({future_time_point}) ===\")\n",
    "\n",
    "        time_point_dir = os.path.join(data_dir, f\"time_point_{i}\")\n",
    "        os.makedirs(time_point_dir, exist_ok=True)\n",
    "\n",
    "        print(\"\\n--- Step 1: Predicting individual future actions ---\")\n",
    "        future_actions_file = os.path.join(time_point_dir, 'future_actions_posts.json')\n",
    "        process_and_save_individual_future_actions(\n",
    "            physical_state_file=physical_state_file,\n",
    "            psychological_state_file=psychological_state_file,\n",
    "            current_time_point=current_time_point,\n",
    "            posts_file=input_posts_file,\n",
    "            future_time_point=future_time_point,\n",
    "            physical_attributes=physical_questions,\n",
    "            output_file=future_actions_file\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Step 2: Forecasting overall future physical state ---\")\n",
    "        future_physical_state_file = os.path.join(time_point_dir, f'future_overall_physical_state_{future_time_point}.json')\n",
    "        process_forecast_overall_future_state_with_io(\n",
    "            input_file=future_actions_file,\n",
    "            physical_attributes=physical_questions,\n",
    "            future_time_point=future_time_point,\n",
    "            current_time_point=current_time_point,\n",
    "            output_file=future_physical_state_file\n",
    "        )\n",
    "        # print(f\"Future physical state saved to {future_physical_state_file}\")\n",
    "\n",
    "        print(\"\\n--- Step 3: Simulating future individual behavior ---\")\n",
    "        simulated_future_posts_file = os.path.join(time_point_dir, f'simulated_future_posts_{future_time_point}.json')\n",
    "        process_and_save_simulated_future_behavior(\n",
    "            physical_state_file=future_physical_state_file,\n",
    "            current_time_point=future_time_point,\n",
    "            input_paths=[input_posts_file],\n",
    "            output_file=simulated_future_posts_file,\n",
    "            usage_log_file='usage_log.json'\n",
    "        )\n",
    "        # print(f\"Simulated future posts saved to {simulated_future_posts_file}\")\n",
    "\n",
    "        print(\"\\n--- Step 4: Deriving psychological overall state ---\")\n",
    "        psychological_state_file = os.path.join(time_point_dir, f'psychological_overall_state_{future_time_point}.json')\n",
    "        process_overall_state(\n",
    "            input_data=simulated_future_posts_file,\n",
    "            questions=psychological_questions,\n",
    "            output_file=psychological_state_file\n",
    "        )\n",
    "        # print(f\"Psychological overall state saved to {psychological_state_file}\")\n",
    "\n",
    "        # Update for next iteration\n",
    "        input_posts_file = simulated_future_posts_file\n",
    "        physical_state_file = future_physical_state_file\n",
    "        current_time_point = future_time_point\n",
    "\n",
    "    print(\"\\n=== Final Step: Answering the user's question ===\")\n",
    "    question = topic\n",
    "    final_physical_state_file = physical_state_file  # Last updated physical state file\n",
    "    answer_output_file = os.path.join(data_dir, 'summary_answer.txt')\n",
    "\n",
    "    process_and_save_summarized_prediction(\n",
    "        question=question,\n",
    "        world_state_file=final_physical_state_file,\n",
    "        current_time_point=current_time_point,\n",
    "        output_file=answer_output_file,\n",
    "        usage_log_file='usage_log.json'\n",
    "    )\n",
    "\n",
    "    print(\"\\nAnswer:\")\n",
    "    with open(answer_output_file, 'r', encoding='utf-8') as f:\n",
    "        answer = f.read()\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544b54ee-4c24-4c15-8c8c-bcf9f8c14801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a list of questions and their corresponding data directories\n",
    "question_directory_map = {\n",
    "    \"What level of development will artificial intelligence reach in the United States by 2050?\": \"tmp/AI/\",\n",
    "    \"What level of advancement will SpaceX’s space technology reach by 2050?\": \"tmp/spacex/\",\n",
    "    \"What will the political polarization between the two parties in the United States develop into by 2050?\": \"tmp/politics/\",\n",
    "    # Add more questions and directories as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed498260-ec60-41ba-b6fb-d813ecc0054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1: Predicting future sequence ===\n",
      "Time points: ['2024', '2028', '2033', '2038', '2043', '2048', '2050']\n",
      "\n",
      "=== Step 2: Getting subreddits and news keywords ===\n",
      "\n",
      "=== Step 3: Crawling data ===\n",
      "Failed to download data from subreddit 'artificialintelligence' sorted by 'hot'. Error: received 404 HTTP response\n",
      "Failed to download data from subreddit 'artificialintelligence' sorted by 'new'. Error: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/0740618d5c75qdxcltvz3vc00000gn/T/ipykernel_71102/1461288327.py:181: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  \"Created Date\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 4: Combining and selecting data ===\n",
      "Combined posts saved to tmp/AI/combined_posts.json\n",
      "\n",
      "=== Step 5: Loading combined posts from file ===\n",
      "\n",
      "=== Step 6: Extracting initial individual contributions ===\n",
      "\n",
      "=== Step 7: Deriving initial overall state ===\n",
      "[longcontext] Tokens used - Prompt: 151773, Candidates: 1443, Cached Content: 151510\n",
      "[longcontext] Tokens used - Prompt: 151852, Candidates: 804, Cached Content: 151510\n",
      "\n",
      "=== Processing Time Point 1/6 (2028) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 920496, Candidates: 392\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 903916, Candidates: 874\n",
      "\n",
      "=== Processing Time Point 2/6 (2033) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 899399, Candidates: 518\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 897272, Candidates: 548\n",
      "\n",
      "=== Processing Time Point 3/6 (2038) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 899039, Candidates: 486\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 900429, Candidates: 525\n",
      "\n",
      "=== Processing Time Point 4/6 (2043) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 901859, Candidates: 477\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 900399, Candidates: 492\n",
      "\n",
      "=== Processing Time Point 5/6 (2048) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 900169, Candidates: 473\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 904571, Candidates: 402\n",
      "\n",
      "=== Processing Time Point 6/6 (2050) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 902755, Candidates: 475\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 898744, Candidates: 388\n",
      "\n",
      "=== Final Step: Answering the user's question ===\n",
      "API call to 'summarize_prediction' completed. Tokens used - Input: 629, Output: 264, Total: 893\n",
      "Summary answer saved to tmp/AI/summary_answer.txt\n",
      "\n",
      "Answer:\n",
      "By 2050, artificial intelligence in the United States has reached pervasive integration and advanced capability, significantly transforming society.  While not achieving true sentience, AI exhibits sophisticated problem-solving across numerous sectors.  This is evidenced by ubiquitous deployment in healthcare (80% of hospitals using AI diagnostics, 100 million telehealth patients), education (70% of schools using AI platforms), finance ($220 trillion in AI-driven trading), and infrastructure (70% AI-managed).  Massive investment ($230 trillion private, $35 billion federal) fuels a robust AI industry (30,000 startups, 2.5 million patents) supported by substantial computing power (170 ExaFLOPS).  Despite regulatory efforts (400 AI-specific laws), concerns persist regarding cybersecurity incidents (120,000 reported) and data privacy (3,000 lawsuits).  The impact on employment remains relatively low (3.8% unemployment in impacted industries).  International collaboration (500 joint projects) and continued military application ($32 billion spending) further underscore AI's central role in shaping the future.  In short, AI has become a powerful, if not perfectly controlled, force shaping American life.\n"
     ]
    }
   ],
   "source": [
    "selection = 1 # What level of development will artificial intelligence reach in the United States by 2050?\n",
    "selected_question = list(question_directory_map.keys())[selection - 1]\n",
    "data_dir = question_directory_map[selected_question]\n",
    "topic = selected_question\n",
    "psychohistory(data_dir, topic, crawl_data=True, filter_data=False, use_existing_sequence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c71c32a1-a72e-48e8-83ab-7783d148dbc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1: Predicting future sequence ===\n",
      "Time points: ['2024-12-01', '2028', '2032', '2036', '2040', '2044', '2050']\n",
      "\n",
      "=== Step 2: Getting subreddits and news keywords ===\n",
      "\n",
      "=== Step 3: Crawling data ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/0740618d5c75qdxcltvz3vc00000gn/T/ipykernel_71102/1461288327.py:181: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  \"Created Date\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download data from subreddit 'commercialspaceflight' sorted by 'hot'. Error: received 404 HTTP response\n",
      "Failed to download data from subreddit 'commercialspaceflight' sorted by 'new'. Error: received 404 HTTP response\n",
      "Failed to download data from subreddit 'eli5' sorted by 'hot'. Error: received 403 HTTP response\n",
      "Failed to download data from subreddit 'eli5' sorted by 'new'. Error: received 403 HTTP response\n",
      "\n",
      "=== Step 4: Combining and selecting data ===\n",
      "Combined posts saved to tmp/spacex/combined_posts.json\n",
      "\n",
      "=== Step 5: Loading combined posts from file ===\n",
      "\n",
      "=== Step 6: Extracting initial individual contributions ===\n",
      "\n",
      "=== Step 7: Deriving initial overall state ===\n",
      "[longcontext] Tokens used - Prompt: 167811, Candidates: 2306, Cached Content: 167575\n",
      "[longcontext] Tokens used - Prompt: 167836, Candidates: 1326, Cached Content: 167575\n",
      "\n",
      "=== Processing Time Point 1/6 (2028) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 865675, Candidates: 357\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "An error occurred: list index out of range\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 900765, Candidates: 354\n",
      "\n",
      "=== Processing Time Point 2/6 (2032) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 909987, Candidates: 498\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 900303, Candidates: 409\n",
      "\n",
      "=== Processing Time Point 3/6 (2036) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 903227, Candidates: 860\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 895679, Candidates: 362\n",
      "\n",
      "=== Processing Time Point 4/6 (2040) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 899300, Candidates: 359\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 899767, Candidates: 365\n",
      "\n",
      "=== Processing Time Point 5/6 (2044) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 900109, Candidates: 534\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "An error occurred: 500 Internal error encountered.\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 899498, Candidates: 315\n",
      "\n",
      "=== Processing Time Point 6/6 (2050) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 899600, Candidates: 359\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 901554, Candidates: 226\n",
      "\n",
      "=== Final Step: Answering the user's question ===\n",
      "API call to 'summarize_prediction' completed. Tokens used - Input: 512, Output: 114, Total: 626\n",
      "Summary answer saved to tmp/spacex/summary_answer.txt\n",
      "\n",
      "Answer:\n",
      "SpaceX, by 2050, transitioned from a launch provider to a vertically integrated space industrial behemoth.  They dominate reusable launch, orbital infrastructure, in-space resource utilization, and interplanetary transport.  Their reusable architecture, refined over hundreds of launches, drives down costs, enabling the burgeoning space economy evidenced by the trillion-dollar space tourism industry and extensive private space investment.  While no longer the sole player, their legacy is the normalization of space access, paving the way for widespread industrialization and the beginnings of a true spacefaring civilization.\n"
     ]
    }
   ],
   "source": [
    "selection = 2 # What level of advancement will SpaceX’s space technology reach by 2050?\n",
    "selected_question = list(question_directory_map.keys())[selection - 1]\n",
    "data_dir = question_directory_map[selected_question]\n",
    "topic = selected_question\n",
    "psychohistory(data_dir, topic, crawl_data=True, filter_data=False, use_existing_sequence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee29ac58-086e-4acb-84f2-f71dd2f0ebb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1: Predicting future sequence ===\n",
      "Time points: ['2024', '2028', '2033', '2038', '2043', '2048', '2050']\n",
      "\n",
      "=== Step 2: Getting subreddits and news keywords ===\n",
      "\n",
      "=== Step 3: Crawling data ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/0740618d5c75qdxcltvz3vc00000gn/T/ipykernel_71102/1461288327.py:181: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  \"Created Date\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 4: Combining and selecting data ===\n",
      "Combined posts saved to tmp/politics/combined_posts.json\n",
      "\n",
      "=== Step 5: Loading combined posts from file ===\n",
      "\n",
      "=== Step 6: Extracting initial individual contributions ===\n",
      "\n",
      "=== Step 7: Deriving initial overall state ===\n",
      "[longcontext] Tokens used - Prompt: 196240, Candidates: 924, Cached Content: 195986\n",
      "[longcontext] Tokens used - Prompt: 196328, Candidates: 709, Cached Content: 195986\n",
      "\n",
      "=== Processing Time Point 1/6 (2028) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 874047, Candidates: 630\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 901121, Candidates: 516\n",
      "\n",
      "=== Processing Time Point 2/6 (2033) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 900548, Candidates: 708\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 897265, Candidates: 599\n",
      "\n",
      "=== Processing Time Point 3/6 (2038) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 900385, Candidates: 823\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 900422, Candidates: 683\n",
      "\n",
      "=== Processing Time Point 4/6 (2043) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 899001, Candidates: 715\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 901468, Candidates: 469\n",
      "\n",
      "=== Processing Time Point 5/6 (2048) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 899382, Candidates: 819\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 898154, Candidates: 297\n",
      "\n",
      "=== Processing Time Point 6/6 (2050) ===\n",
      "\n",
      "--- Step 1: Predicting individual future actions ---\n",
      "\n",
      "--- Step 2: Forecasting overall future physical state ---\n",
      "[longcontext] Tokens used - Prompt: 900738, Candidates: 675\n",
      "\n",
      "--- Step 3: Simulating future individual behavior ---\n",
      "\n",
      "--- Step 4: Deriving psychological overall state ---\n",
      "[longcontext] Tokens used - Prompt: 898939, Candidates: 429\n",
      "\n",
      "=== Final Step: Answering the user's question ===\n",
      "API call to 'summarize_prediction' completed. Tokens used - Input: 832, Output: 266, Total: 1098\n",
      "Summary answer saved to tmp/politics/summary_answer.txt\n",
      "\n",
      "Answer:\n",
      "Political polarization in the United States has solidified into a persistent, low-turnout stalemate.  While near parity exists in party identification (41% Republican, 40% Democrat),  high levels of partisan gerrymandering (28% of districts) and massive campaign finance expenditures ($9.2B total, with minimal independent spending) reinforce existing divisions. This, coupled with pervasive misinformation (affecting 52% of fact-checked news) and an extremely concentrated media landscape (HHI 1620),  creates echo chambers that deepen ideological divides.  While voter turnout has marginally improved across all affiliations (61-64%), it isn't enough to overcome structural advantages.  The result is a narrowly divided government prone to gridlock, reflected in low approval ratings for national institutions (President: 35%, Congress: 28%).  Increasingly, political action manifests not through electoral change, but through lobbying ($9B) and, disturbingly,  a rise in politically motivated violence (750 incidents).  This entrenched polarization overshadows pressing issues like climate change (1.2°C warming, 95 extreme weather events) and economic inequality (Gini 0.44),  hindering meaningful solutions and fueling public disillusionment.\n"
     ]
    }
   ],
   "source": [
    "selection = 3 # What will the political polarization between the two parties in the United States develop into by 2050?\n",
    "selected_question = list(question_directory_map.keys())[selection - 1]\n",
    "data_dir = question_directory_map[selected_question]\n",
    "topic = selected_question\n",
    "psychohistory(data_dir, topic, crawl_data=True, filter_data=False, use_existing_sequence=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d538556-fccc-415d-a4e7-b00ae6e39e9c",
   "metadata": {},
   "source": [
    "Psychohistory’s predictions about AI:\n",
    "Projected Societal Attributes for the United States in 2050:\n",
    "\t1.\tUS Federal Funding Allocated to AI Research (in USD): $35 Billion\n",
    "\t2.\tPrivate Investment in US AI Companies (in USD): $230 Trillion\n",
    "\t3.\tNumber of AI-Related Patents Granted by the USPTO: 2,500,000\n",
    "\t4.\tNumber of Active AI Startups in the US: 30,000\n",
    "\t5.\tAvailable Computing Power (measured in FLOPS, specifically dedicated to AI research in the US): 170 ExaFLOPS\n",
    "\t6.\tNumber of Reported Cybersecurity Incidents Related to AI in the US: 120,000\n",
    "\t7.\tNumber of Graduates in AI-Related Fields from US Universities: 250,000\n",
    "\t8.\tUnemployment Rate in US Industries Impacted by AI Automation: 3.8%\n",
    "\t9.\tNumber of AI-Specific Regulations Passed by US Congress: 400\n",
    "\t10.\tUS Military Spending on AI Systems (in USD): $32 Billion\n",
    "\t11.\tPercentage of US Hospitals Utilizing AI-Powered Diagnostic Tools: 80%\n",
    "\t12.\tEnergy Consumption by Data Centers Supporting AI Development in the US (in kWh): 380 Billion kWh\n",
    "\t13.\tNumber of AI-Related Data Privacy Lawsuits Filed in the US: 3,000\n",
    "\t14.\tPercentage of US Critical Infrastructure Utilizing AI-Driven Management Systems: 70%\n",
    "\t15.\tNumber of Joint US-International AI Research Projects: 500\n",
    "\t16.\tNumber of Industrial Robots Deployed in US Manufacturing: 2,000,000\n",
    "\t17.\tPercentage of US Schools Implementing AI-Powered Learning Platforms: 70%\n",
    "\t18.\tNumber of Patients Using AI-Driven Telehealth Services in the US: 100 Million\n",
    "\t19.\tTrading Volume on US Stock Exchanges Attributed to AI-Driven Algorithms (in USD): $220 Trillion\n",
    "\t20.\tPercentage of US Households with Access to Broadband Internet: 99%\n",
    "Key Points Summary:\n",
    "\t1.\tAI Integration in Society:\n",
    "\t•\tHealthcare: 80% of hospitals use AI diagnostics; 100 million patients depend on AI-driven telehealth.\n",
    "\t•\tEducation: 70% of schools employ AI-powered learning platforms.\n",
    "\t•\tEconomy: Over 2.5 million AI-related patents, 30,000 active AI startups, and a growing workforce of AI-specialized graduates.\n",
    "\t•\tConnectivity: 99% of US households have broadband access.\n",
    "\t2.\tEconomic and Social Challenges:\n",
    "\t•\tUnemployment: Official rate is 3.8%, but many face AI-driven job displacement, leading to more gig work and a wider wealth gap.\n",
    "\t•\tRegulatory and Ethical Issues: Despite 400 AI-specific regulations, concerns about data privacy, algorithmic bias, and 3,000 annual AI-related lawsuits persist.\n",
    "\t3.\tCybersecurity and National Defense:\n",
    "\t•\tThreats: Increasing reliance on AI in critical infrastructure poses significant cybersecurity risks.\n",
    "\t•\tMilitary: $32 billion invested in AI, raising ethical concerns about autonomous weaponry.\n",
    "\t4.\tEnvironmental Concerns:\n",
    "\t•\tAI data centers consume 380 billion kWh annually, demanding urgent transition to sustainable energy solutions.\n",
    "\t5.\tPower Dynamics and Ethical Implications:\n",
    "\t•\tConcentration of AI power in a few tech giants creates uncertainty and anxiety.\n",
    "\t•\tCalls for stronger ethical frameworks, international collaboration, and a focus on societal equity.\n",
    "\t6.\tFuture Outlook:\n",
    "\t•\tThe future of AI hinges on current decisions, with the potential to either greatly benefit humanity or lead to serious consequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
